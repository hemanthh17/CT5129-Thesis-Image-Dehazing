{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a7efba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T12:36:25.197379Z",
     "iopub.status.busy": "2024-07-12T12:36:25.196958Z",
     "iopub.status.idle": "2024-07-12T12:36:33.611980Z",
     "shell.execute_reply": "2024-07-12T12:36:33.610808Z"
    },
    "papermill": {
     "duration": 8.423314,
     "end_time": "2024-07-12T12:36:33.614785",
     "exception": false,
     "start_time": "2024-07-12T12:36:25.191471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc,os,cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pywt\n",
    "import shutil,time\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as tt \n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76370ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T12:36:33.624836Z",
     "iopub.status.busy": "2024-07-12T12:36:33.623577Z",
     "iopub.status.idle": "2024-07-12T12:36:33.630380Z",
     "shell.execute_reply": "2024-07-12T12:36:33.629311Z"
    },
    "papermill": {
     "duration": 0.014128,
     "end_time": "2024-07-12T12:36:33.632830",
     "exception": false,
     "start_time": "2024-07-12T12:36:33.618702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_transforms_rgb=tt.Compose([\n",
    "    tt.transforms.Resize((256,256),antialias=True),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(mean=(0.6344,0.5955,0.5857),std=(0.1742,0.1798,0.1871))\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ee4c7",
   "metadata": {
    "papermill": {
     "duration": 0.00315,
     "end_time": "2024-07-12T12:36:33.639496",
     "exception": false,
     "start_time": "2024-07-12T12:36:33.636346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Initilaizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bfed51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T12:36:33.647995Z",
     "iopub.status.busy": "2024-07-12T12:36:33.647627Z",
     "iopub.status.idle": "2024-07-12T12:36:33.678049Z",
     "shell.execute_reply": "2024-07-12T12:36:33.676874Z"
    },
    "papermill": {
     "duration": 0.037868,
     "end_time": "2024-07-12T12:36:33.680751",
     "exception": false,
     "start_time": "2024-07-12T12:36:33.642883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduct_ratio=8):\n",
    "        super(PixelAttention,self).__init__()\n",
    "        reduced_channel=max(1,channel//reduct_ratio)\n",
    "        self.pixel_attention=nn.Sequential(\n",
    "            nn.Conv2d(channel,channel//reduced_channel,kernel_size=1,padding=0,bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel//reduced_channel,1,kernel_size=1,padding=0,bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,feature):\n",
    "        x=self.pixel_attention(feature)\n",
    "        return x*feature\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,input_channels,reduct_ratio=8):\n",
    "        super(ChannelAttention,self).__init__()\n",
    "        reduced_channel=max(1,input_channels//reduct_ratio)\n",
    "        self.avg_pooler=nn.AdaptiveAvgPool2d(1)\n",
    "        self.fcn=nn.Sequential(\n",
    "            nn.Linear(input_channels,reduced_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(reduced_channel,input_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self,input_feature):\n",
    "        n,c,_,_=input_feature.size()\n",
    "        x=self.avg_pooler(input_feature).view(n,c)\n",
    "        x=F.sigmoid(self.fcn(x).view(n,c,1,1))\n",
    "        return input_feature*x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,dims,kernel_size=1):\n",
    "        super(AttentionBlock,self).__init__()\n",
    "        self.conv1=nn.Conv2d(dims,dims,kernel_size,padding=(kernel_size//2),bias=True)\n",
    "        self.conv2=nn.Conv2d(dims,dims,kernel_size,padding=(kernel_size//2),bias=True)\n",
    "        self.ca=ChannelAttention(dims)\n",
    "        self.pa=PixelAttention(dims)\n",
    "    def forward(self,img):\n",
    "        feat=F.relu(self.conv1(img),inplace=True)\n",
    "        feat=feat+img\n",
    "        feat=F.relu(self.conv1(feat),inplace=True)\n",
    "        feat=self.ca(feat)\n",
    "        feat=self.pa(feat)\n",
    "        feat+=img\n",
    "        return feat\n",
    "class DWT_DehazingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DWT_DehazingNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.attn1=AttentionBlock(3)\n",
    "        self.conv3=nn.Conv2d(in_channels=9,out_channels=3,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv4=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=7,stride=1,padding=3)\n",
    "        self.attn2=AttentionBlock(3)\n",
    "        self.conv5=nn.Conv2d(in_channels=15,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_dwt=nn.Conv2d(in_channels=12,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.b=1\n",
    "\n",
    "    def forward(self,x):\n",
    "        dwt_coeffs=pywt.dwt2(x.cpu(),wavelet='db4')\n",
    "        LL,(LH,HL,HH)=dwt_coeffs\n",
    "        dwt_out=torch.concat([torch.from_numpy(LL),torch.from_numpy(LH),torch.from_numpy(HL),torch.from_numpy(HH)],dim=1)\n",
    "        x1=F.relu(self.conv1(x))\n",
    "        dwt_out=tt.Resize((256,256))(dwt_out)\n",
    "        dwt_in=self.conv_dwt(dwt_out)\n",
    "        x2=F.relu(self.conv2(x1))\n",
    "        x2=self.attn1(x2)\n",
    "        cat1=torch.cat((x1,x2,dwt_in),1)\n",
    "        x3=F.relu(self.conv3(cat1))\n",
    "        cat2=torch.cat((x2,x3),1)\n",
    "        x4=F.relu(self.conv4(cat2))\n",
    "        x4=self.attn2(x4)\n",
    "        cat3=torch.cat((x1,x2,x3,x4,dwt_in),1)\n",
    "        k=F.relu(self.conv5(cat3))\n",
    "        return F.relu(k*x-k+self.b)\n",
    "class DehazingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DehazingNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.attn1=AttentionBlock(3)\n",
    "        self.conv3=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv4=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=7,stride=1,padding=3)\n",
    "        self.attn2=AttentionBlock(3)\n",
    "        self.conv5=nn.Conv2d(in_channels=12,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_dwt=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.b=1\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1=F.relu(self.conv1(x))\n",
    "        x2=F.relu(self.conv2(x1))\n",
    "        x2=self.attn1(x2)\n",
    "        cat1=torch.cat((x1,x2),1)\n",
    "        x3=F.relu(self.conv3(cat1))\n",
    "        cat2=torch.cat((x2,x3),1)\n",
    "        x4=F.relu(self.conv4(cat2))\n",
    "        x4=self.attn2(x4)\n",
    "        cat3=torch.cat((x1,x2,x3,x4),1)\n",
    "        k=F.relu(self.conv5(cat3))\n",
    "        return F.relu(k*x-k+self.b)\n",
    "class FinalCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FinalCNN,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv3=nn.Conv2d(in_channels=32,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.relu3=nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.relu1(self.conv1(x))\n",
    "        x=self.relu2(self.conv2(x))\n",
    "        x=self.relu3(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de50c55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T12:36:33.689517Z",
     "iopub.status.busy": "2024-07-12T12:36:33.689117Z",
     "iopub.status.idle": "2024-07-12T12:36:33.772027Z",
     "shell.execute_reply": "2024-07-12T12:36:33.770789Z"
    },
    "papermill": {
     "duration": 0.090103,
     "end_time": "2024-07-12T12:36:33.774508",
     "exception": false,
     "start_time": "2024-07-12T12:36:33.684405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwt_dehazenet_rgb=nn.DataParallel(DWT_DehazingNet())\n",
    "dehazenet_rgb=nn.DataParallel(DehazingNet())\n",
    "final_cnn=FinalCNN()\n",
    "\n",
    "\n",
    "dwt_dehazenet_rgb.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/dehazing-rgb-dwt-2l.pth',map_location=torch.device('cpu')))\n",
    "dehazenet_rgb.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/dehazenet-rgb-2l.pth',map_location=torch.device('cpu')))\n",
    "final_cnn.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/end_cnn.pth',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c619b34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T12:36:33.783840Z",
     "iopub.status.busy": "2024-07-12T12:36:33.783459Z",
     "iopub.status.idle": "2024-07-12T12:36:33.801592Z",
     "shell.execute_reply": "2024-07-12T12:36:33.800456Z"
    },
    "papermill": {
     "duration": 0.025741,
     "end_time": "2024-07-12T12:36:33.803932",
     "exception": false,
     "start_time": "2024-07-12T12:36:33.778191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_image_rgb(img_tensor,file_path):\n",
    "    if img_tensor.shape[0]!=3:\n",
    "        raise ValueError(\"Input tensor must have 3 channels only...\")\n",
    "    rgb_array=tensor_denormalize_rgb(img_tensor).permute(1,2,0).cpu().detach().numpy()\n",
    "    rgb_image=Image.fromarray((np.clip(rgb_array,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    rgb_image.save(file_path)\n",
    "\n",
    "def tensor_denormalize_rgb(out_tensor,mean=[0.4556,0.3837,0.3642],std=[0.2689,0.2691,0.2828]):\n",
    "    if len(out_tensor.shape)==3:\n",
    "        out_tensor=out_tensor.unsqueeze(0)\n",
    "    mean=torch.tensor(mean).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "    std=torch.tensor(std).unsqueeze(0).unsqueeze(2).unsqueeze(3)    \n",
    "    denorm_tensor=(out_tensor*std)+mean\n",
    "    return denorm_tensor.squeeze(0)\n",
    "def unsharp_mask(image,kernel_size=(5,5),sigma=0.4,amount=1.0,threshold=1):\n",
    "    blurred=cv2.GaussianBlur(image,kernel_size,sigma)\n",
    "    sharpened=float(amount+1)*image-float(amount)*blurred\n",
    "    sharpened=np.maximum(sharpened,np.zeros(sharpened.shape))\n",
    "    sharpened=np.minimum(sharpened,255*np.ones(sharpened.shape))\n",
    "    sharpened=sharpened.round().astype(np.uint8)\n",
    "    if threshold>0:\n",
    "        low_contrast_mask=np.absolute(image-blurred)<threshold\n",
    "        np.copyto(sharpened,image,where=low_contrast_mask)\n",
    "    return sharpened\n",
    "\n",
    "def clahe(image):\n",
    "    clahe=cv2.createCLAHE(clipLimit=1,tileGridSize=(2,2))\n",
    "    lab=cv2.cvtColor(image,cv2.COLOR_BGR2LAB)\n",
    "    lab[:,:,0]=clahe.apply(lab[:,:,0])\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def enhance_image(image_path):\n",
    "    image=cv2.imread(image_path)\n",
    "    img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_sharpened=unsharp_mask(img)\n",
    "    image_clahe=clahe(image_sharpened)\n",
    "    image_tensor=tt.ToTensor()(image_clahe)   \n",
    "    return image_tensor\n",
    "def alpha_blending(image1, image2, alpha=0.6):\n",
    "    blended = cv2.addWeighted(image1, alpha, image2, 1 - alpha, 4)\n",
    "    return blended\n",
    "def image_addition(coeff,img_path1,img_path2):\n",
    "    img1=cv2.cvtColor(cv2.imread(img_path1),cv2.COLOR_BGR2RGB)\n",
    "    img2=cv2.cvtColor(cv2.imread(img_path2),cv2.COLOR_BGR2RGB)\n",
    "    img_f=tt.ToTensor()(alpha_blending(img1,img2))    \n",
    "    return img_f\n",
    "def save_image_final(img_tensor,file_path):\n",
    "    if img_tensor.shape[0]!=3:\n",
    "        raise ValueError(\"Input tensor must have 3 channels only...\")\n",
    "    rgb_array=img_tensor.permute(1,2,0).cpu().detach().numpy()\n",
    "    rgb_image=Image.fromarray((np.clip(rgb_array,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    rgb_image.save(file_path)\n",
    "def enhance_image_merged(image_path):\n",
    "    image=cv2.imread(image_path)\n",
    "    img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img2=cv2.fastNlMeansDenoisingColored(img,None,10,10,3,21)\n",
    "    image_sharpened=unsharp_mask(img2)\n",
    "    image_clahe=clahe(image_sharpened)\n",
    "    image_tensor=tt.ToTensor()(image_clahe)   \n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cde8917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T12:36:33.812684Z",
     "iopub.status.busy": "2024-07-12T12:36:33.812275Z",
     "iopub.status.idle": "2024-07-12T12:36:33.821338Z",
     "shell.execute_reply": "2024-07-12T12:36:33.820252Z"
    },
    "papermill": {
     "duration": 0.016139,
     "end_time": "2024-07-12T12:36:33.823672",
     "exception": false,
     "start_time": "2024-07-12T12:36:33.807533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model1_image_pass(img_tensor,folder_name,i):\n",
    "    model_output1=dwt_dehazenet_rgb(img_tensor.unsqueeze(0)).cpu()\n",
    "    save_image_rgb(model_output1.squeeze(),f'/kaggle/working/{folder_name}/output_image_{i}_1.png')\n",
    "    proc_out_tensor1=enhance_image(f'/kaggle/working/{folder_name}/output_image_{i}_1.png')\n",
    "    proc_arr1=proc_out_tensor1.permute(1,2,0).cpu().detach().numpy()\n",
    "    proc_image1=Image.fromarray((np.clip(proc_arr1,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    proc_image1.save(f'/kaggle/working/{folder_name}/processed_image_{i}_1.png')\n",
    "    return proc_arr1\n",
    "    \n",
    "    \n",
    "def model2_image_pass(img_tensor,folder_name,i):\n",
    "    model_output2=dehazenet_rgb(img_tensor.unsqueeze(0)).cpu()\n",
    "    save_image_rgb(model_output2.squeeze(),f'/kaggle/working/{folder_name}/output_image_{i}_2.png')\n",
    "    proc_out_tensor2=enhance_image(f'/kaggle/working/{folder_name}/output_image_{i}_2.png')\n",
    "    proc_arr2=proc_out_tensor2.permute(1,2,0).cpu().detach().numpy()\n",
    "    proc_image2=Image.fromarray((np.clip(proc_arr2,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    proc_image2.save(f'/kaggle/working/{folder_name}/processed_image_{i}_2.png')\n",
    "    return proc_arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520d7de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T12:36:33.833337Z",
     "iopub.status.busy": "2024-07-12T12:36:33.832270Z",
     "iopub.status.idle": "2024-07-12T12:36:38.626876Z",
     "shell.execute_reply": "2024-07-12T12:36:38.625580Z"
    },
    "papermill": {
     "duration": 4.80218,
     "end_time": "2024-07-12T12:36:38.629571",
     "exception": false,
     "start_time": "2024-07-12T12:36:33.827391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def model_run_parallel(haze_img_path,i=0,folder_name='Inference_Results'):\n",
    "    os.makedirs(f'/kaggle/working/{folder_name}',exist_ok=True)\n",
    "    dwt_dehazenet_rgb.eval()\n",
    "    dwt_dehazenet_rgb.eval()\n",
    "    start_time=time.time()\n",
    "    haze_img_tensor=cv2.cvtColor(cv2.imread(haze_img_path),cv2.COLOR_BGR2RGB)\n",
    "    haze_img_tensor=Image.open(haze_img_path)\n",
    "    img_tensor=input_transforms_rgb(haze_img_tensor).cpu()\n",
    "    \n",
    "    arr1,arr2=Parallel(n_jobs=2)(delayed(func)(img_tensor,folder_name,i) for func in [model1_image_pass,model2_image_pass])\n",
    "    blended_image=image_addition(0.6,f'/kaggle/working/{folder_name}/processed_image_{i}_2.png',\n",
    "                                 f'/kaggle/working/{folder_name}/processed_image_{i}_1.png')\n",
    "    save_image_final(blended_image.squeeze(),f'/kaggle/working/{folder_name}/merged_image_{i}.png')\n",
    "    proc_blended=enhance_image_merged(f'/kaggle/working/{folder_name}/merged_image_{i}.png')\n",
    "    save_image_final(blended_image.squeeze(),f'/kaggle/working/{folder_name}/proc_merged_image_{i}.png')\n",
    "    cnn_processed=final_cnn(tt.ToTensor()(cv2.cvtColor(cv2.imread(f'/kaggle/working/{folder_name}/proc_merged_image_{i}.png'),\n",
    "                                                       cv2.COLOR_BGR2RGB)))\n",
    "    save_image_final(cnn_processed.squeeze(),f'/kaggle/working/{folder_name}/cnn_processed_image_{i}.png')\n",
    "\n",
    "model_run_parallel('/kaggle/input/dehazing-dataset-thesis/NH-HAZE/NH-HAZE/01_hazy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9200ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T12:36:38.639037Z",
     "iopub.status.busy": "2024-07-12T12:36:38.638641Z",
     "iopub.status.idle": "2024-07-12T12:42:13.907644Z",
     "shell.execute_reply": "2024-07-12T12:42:13.906270Z"
    },
    "papermill": {
     "duration": 335.279729,
     "end_time": "2024-07-12T12:42:13.913081",
     "exception": false,
     "start_time": "2024-07-12T12:36:38.633352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed....\n",
      "Average workflow processing time: 0.6446901701963865\n"
     ]
    }
   ],
   "source": [
    "test_data=pd.read_csv('/kaggle/input/dehazing-dataset-thesis/dehazing_dataset_test.csv')\n",
    "time_data=[]\n",
    "for idx,(path) in enumerate(test_data.Hazy.values):\n",
    "    start_time=time.time()\n",
    "    model_run_parallel(path,idx)\n",
    "    time_data.append(time.time()-start_time)\n",
    "print('Processed....')\n",
    "print(f\"Average workflow processing time: {sum(time_data)/len(time_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741fce8",
   "metadata": {
    "papermill": {
     "duration": 0.00376,
     "end_time": "2024-07-12T12:42:13.920663",
     "exception": false,
     "start_time": "2024-07-12T12:42:13.916903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5101434,
     "sourceId": 8729607,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5239863,
     "sourceId": 8857586,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 354.311228,
   "end_time": "2024-07-12T12:42:16.545917",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-12T12:36:22.234689",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
