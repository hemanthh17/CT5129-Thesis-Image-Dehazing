{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d14a7f4",
   "metadata": {},
   "source": [
    "# DWT and DehazingNet Model \n",
    "The DWT with DehazingNet model is trained using the Mean Squared error and Perceptual Loss. \n",
    "\n",
    "## Installing Additional Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6ef890",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-29T00:54:58.535583Z",
     "iopub.status.busy": "2024-06-29T00:54:58.535234Z",
     "iopub.status.idle": "2024-06-29T00:55:12.587122Z",
     "shell.execute_reply": "2024-06-29T00:55:12.586036Z"
    },
    "papermill": {
     "duration": 14.061314,
     "end_time": "2024-06-29T00:55:12.589456",
     "exception": false,
     "start_time": "2024-06-29T00:54:58.528142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install wandb torch-enhance torchmetrics lpips -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa2064",
   "metadata": {
    "papermill": {
     "duration": 0.004921,
     "end_time": "2024-06-29T00:55:12.599981",
     "exception": false,
     "start_time": "2024-06-29T00:55:12.595060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338f10c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:12.611997Z",
     "iopub.status.busy": "2024-06-29T00:55:12.611307Z",
     "iopub.status.idle": "2024-06-29T00:55:24.224551Z",
     "shell.execute_reply": "2024-06-29T00:55:24.223683Z"
    },
    "papermill": {
     "duration": 11.62174,
     "end_time": "2024-06-29T00:55:24.226688",
     "exception": false,
     "start_time": "2024-06-29T00:55:12.604948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc,os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lpips\n",
    "import pywt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as tt \n",
    "from torch_enhance.losses import VGG as PerceptualLoss\n",
    "from torchmetrics.image import PeakSignalNoiseRatio,StructuralSimilarityIndexMeasure\n",
    "\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets=UserSecretsClient()\n",
    "secret_value=user_secrets.get_secret(\"WANDB\")\n",
    "wandb.login(key=secret_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10287a8a",
   "metadata": {},
   "source": [
    "## Setting the Configuration\n",
    "Various hyperparameters are set here which helps to keep the associated settings uniform across the script. To monitor and track the model Weights and Biases has been used. No artifacts are being logged into the server.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6e4126",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:24.239067Z",
     "iopub.status.busy": "2024-06-29T00:55:24.238779Z",
     "iopub.status.idle": "2024-06-29T00:55:40.626991Z",
     "shell.execute_reply": "2024-06-29T00:55:40.625600Z"
    },
    "papermill": {
     "duration": 16.397117,
     "end_time": "2024-06-29T00:55:40.629491",
     "exception": false,
     "start_time": "2024-06-29T00:55:24.232374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhemanthh17\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240629_005524-3855poet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgallant-paper-86\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/hemanthh17/CT5129-Image%20Dehazing\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/hemanthh17/CT5129-Image%20Dehazing/runs/3855poet\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hemanthh17/CT5129-Image%20Dehazing/runs/3855poet?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x79d6e42a2c20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    lr=1e-4\n",
    "    epochs=14\n",
    "    train=True\n",
    "    stats=False\n",
    "    image_shape=(256,256)\n",
    "    train_bs=32\n",
    "    val_bs=4\n",
    "    test_bs=4\n",
    "    es_patience=5\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    #device=\"cpu\"\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"CT5129-Image Dehazing\",\n",
    "    config={\n",
    "    \"learning_rate\": CFG.lr,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"Image Dehazing Dataset\",\n",
    "    \"epochs\": CFG.epochs,\n",
    "     \"training_bs\":CFG.train_bs,\n",
    "        \"validation_bs\":CFG.val_bs,\n",
    "        \"test_bs\":CFG.test_bs,\n",
    "        \"device\":CFG.device,\n",
    "        \"optimizer\":\"Adam\",\n",
    "        \"es_patience\":CFG.es_patience,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1646bb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:40.643966Z",
     "iopub.status.busy": "2024-06-29T00:55:40.643674Z",
     "iopub.status.idle": "2024-06-29T00:55:40.821740Z",
     "shell.execute_reply": "2024-06-29T00:55:40.820750Z"
    },
    "papermill": {
     "duration": 0.187924,
     "end_time": "2024-06-29T00:55:40.824054",
     "exception": false,
     "start_time": "2024-06-29T00:55:40.636130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('/kaggle/input/dehazing-dataset-thesis/dehaing_dataset_train.csv')\n",
    "val_data=pd.read_csv('/kaggle/input/dehazing-dataset-thesis/dehazing_dataset_val.csv')\n",
    "test_data=pd.read_csv('/kaggle/input/dehazing-dataset-thesis/dehazing_dataset_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cecfd8a",
   "metadata": {},
   "source": [
    "## Computing the Image Statistics\n",
    "\n",
    "In this step we compute the channel mean and standard deviation. This will help normalize the image and pass into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f489e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:40.839944Z",
     "iopub.status.busy": "2024-06-29T00:55:40.839325Z",
     "iopub.status.idle": "2024-06-29T00:55:40.849390Z",
     "shell.execute_reply": "2024-06-29T00:55:40.848567Z"
    },
    "papermill": {
     "duration": 0.019155,
     "end_time": "2024-06-29T00:55:40.851231",
     "exception": false,
     "start_time": "2024-06-29T00:55:40.832076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.stats:\n",
    "    img_transformer=tt.transforms.Compose([\n",
    "        tt.transforms.Resize(CFG.image_shape),\n",
    "        tt.transforms.ToTensor()\n",
    "    ])\n",
    "    hazy_img_list=[img_transformer(Image.open(img)).to(CFG.device) for img in train_data.Hazy.values]\n",
    "    hazy_images_stack=torch.stack(hazy_img_list,dim=1)\n",
    "    hazy_images_stack=hazy_images_stack.permute(1,0,2,3)\n",
    "    print(\"Hazy Images Stack Dimension:\",hazy_images_stack.shape)\n",
    "    print(\"Mean of the hazy images per channel:\",torch.mean(hazy_images_stack,dim=(0,2,3)))\n",
    "    print(\"Standard Deviation of the hazy images per channel:\",torch.std(hazy_images_stack,dim=(0,2,3)))\n",
    "    del hazy_img_list,hazy_images_stack\n",
    "    gc.collect()\n",
    "    clear_img_list=[img_transformer(Image.open(img)).to(CFG.device) for img in train_data.GT.values]\n",
    "    clear_images_stack=torch.stack(clear_img_list,dim=1)\n",
    "    clear_images_stack=clear_images_stack.permute(1,0,2,3)\n",
    "    print(\"Clear Images Stack Dimension:\",clear_images_stack.shape)\n",
    "    print(\"Mean of the hazy images per channel:\",torch.mean(clear_images_stack,dim=(0,2,3)))\n",
    "    print(\"Standard Deviation of the hazy images per channel:\",torch.std(clear_images_stack,dim=(0,2,3)))\n",
    "    del clear_img_list,clear_images_stack\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada22322",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:40.864738Z",
     "iopub.status.busy": "2024-06-29T00:55:40.864448Z",
     "iopub.status.idle": "2024-06-29T00:55:40.870098Z",
     "shell.execute_reply": "2024-06-29T00:55:40.869276Z"
    },
    "papermill": {
     "duration": 0.014482,
     "end_time": "2024-06-29T00:55:40.872019",
     "exception": false,
     "start_time": "2024-06-29T00:55:40.857537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_transforms=tt.Compose([\n",
    "    tt.transforms.Resize(CFG.image_shape),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(mean=(0.6344,0.5955,0.5857),std=(0.1742,0.1798,0.1871))\n",
    "])\n",
    "output_transforms=tt.Compose([\n",
    "    tt.transforms.Resize(CFG.image_shape),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(mean=(0.4556,0.3837,0.3642),std=(0.2689,0.2691,0.2828))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f9480",
   "metadata": {},
   "source": [
    "## Creating Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee08100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:40.886030Z",
     "iopub.status.busy": "2024-06-29T00:55:40.885527Z",
     "iopub.status.idle": "2024-06-29T00:55:40.892249Z",
     "shell.execute_reply": "2024-06-29T00:55:40.891454Z"
    },
    "papermill": {
     "duration": 0.015746,
     "end_time": "2024-06-29T00:55:40.894081",
     "exception": false,
     "start_time": "2024-06-29T00:55:40.878335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DehazingDataset(Dataset):\n",
    "    def __init__(self,dataset,in_transforms=None,out_transforms=None):\n",
    "        self.dataset=dataset\n",
    "        self.in_transforms=in_transforms\n",
    "        self.out_transforms=out_transforms\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self,idx):\n",
    "        hazy_img_path=self.dataset.iloc[idx,1]\n",
    "        clear_img_path=self.dataset.iloc[idx,0]\n",
    "        if self.in_transforms:\n",
    "            hazy_img=self.in_transforms(Image.open(str(hazy_img_path)))\n",
    "        if self.out_transforms:\n",
    "            clear_img=self.out_transforms(Image.open(str(clear_img_path)))\n",
    "        return {'hazy':hazy_img,\n",
    "               'gt':clear_img}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d3c2d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:40.907760Z",
     "iopub.status.busy": "2024-06-29T00:55:40.907458Z",
     "iopub.status.idle": "2024-06-29T00:55:40.912764Z",
     "shell.execute_reply": "2024-06-29T00:55:40.911936Z"
    },
    "papermill": {
     "duration": 0.014152,
     "end_time": "2024-06-29T00:55:40.914700",
     "exception": false,
     "start_time": "2024-06-29T00:55:40.900548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset=DehazingDataset(train_data,input_transforms,output_transforms)\n",
    "val_dataset=DehazingDataset(val_data,input_transforms,output_transforms)\n",
    "test_dataset=DehazingDataset(test_data,input_transforms,output_transforms)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=CFG.train_bs)\n",
    "val_loader=DataLoader(val_dataset,batch_size=CFG.val_bs)\n",
    "test_loader=DataLoader(test_dataset,batch_size=CFG.test_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ce402",
   "metadata": {
    "papermill": {
     "duration": 0.00613,
     "end_time": "2024-06-29T00:55:40.927013",
     "exception": false,
     "start_time": "2024-06-29T00:55:40.920883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9356875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:40.941214Z",
     "iopub.status.busy": "2024-06-29T00:55:40.940975Z",
     "iopub.status.idle": "2024-06-29T00:55:40.963397Z",
     "shell.execute_reply": "2024-06-29T00:55:40.962666Z"
    },
    "papermill": {
     "duration": 0.031577,
     "end_time": "2024-06-29T00:55:40.965218",
     "exception": false,
     "start_time": "2024-06-29T00:55:40.933641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduct_ratio=8):\n",
    "        super(PixelAttention,self).__init__()\n",
    "        reduced_channel=max(1,channel//reduct_ratio)\n",
    "        self.pixel_attention=nn.Sequential(\n",
    "            nn.Conv2d(channel,channel//reduced_channel,kernel_size=1,padding=0,bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel//reduced_channel,1,kernel_size=1,padding=0,bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,feature):\n",
    "        x=self.pixel_attention(feature)\n",
    "        return x*feature\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,input_channels,reduct_ratio=8):\n",
    "        super(ChannelAttention,self).__init__()\n",
    "        reduced_channel=max(1,input_channels//reduct_ratio)\n",
    "        self.avg_pooler=nn.AdaptiveAvgPool2d(1)\n",
    "        self.fcn=nn.Sequential(\n",
    "            nn.Linear(input_channels,reduced_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(reduced_channel,input_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self,input_feature):\n",
    "        n,c,_,_=input_feature.size()\n",
    "        x=self.avg_pooler(input_feature).view(n,c)\n",
    "        x=F.sigmoid(self.fcn(x).view(n,c,1,1))\n",
    "        return input_feature*x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,dims,kernel_size=1):\n",
    "        super(AttentionBlock,self).__init__()\n",
    "        self.conv1=nn.Conv2d(dims,dims,kernel_size,padding=(kernel_size//2),bias=True)\n",
    "        self.conv2=nn.Conv2d(dims,dims,kernel_size,padding=(kernel_size//2),bias=True)\n",
    "        self.ca=ChannelAttention(dims)\n",
    "        self.pa=PixelAttention(dims)\n",
    "    def forward(self,img):\n",
    "        feat=F.relu(self.conv1(img),inplace=True)\n",
    "        feat=feat+img\n",
    "        feat=F.relu(self.conv1(feat),inplace=True)\n",
    "        feat=self.ca(feat)\n",
    "        feat=self.pa(feat)\n",
    "        feat+=img\n",
    "        return feat\n",
    "class DehazingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DehazingNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.attn1=AttentionBlock(3)\n",
    "        self.conv3=nn.Conv2d(in_channels=9,out_channels=3,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv4=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=7,stride=1,padding=3)\n",
    "        self.attn2=AttentionBlock(3)\n",
    "        self.conv5=nn.Conv2d(in_channels=15,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_dwt=nn.Conv2d(in_channels=12,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.b=1\n",
    "\n",
    "    def forward(self,x):\n",
    "        dwt_coeffs=pywt.dwt2(x.cpu(),wavelet='db4')\n",
    "        LL,(LH,HL,HH)=dwt_coeffs\n",
    "        dwt_out=torch.concat([torch.from_numpy(LL),torch.from_numpy(LH),torch.from_numpy(HL),torch.from_numpy(HH)],dim=1)\n",
    "        x1=F.relu(self.conv1(x))\n",
    "        dwt_out=tt.Resize((256,256))(dwt_out).to(CFG.device)\n",
    "        dwt_in=self.conv_dwt(dwt_out)\n",
    "        x2=F.relu(self.conv2(x1))\n",
    "        x2=self.attn1(x2)\n",
    "        cat1=torch.cat((x1,x2,dwt_in),1)\n",
    "        x3=F.relu(self.conv3(cat1))\n",
    "        cat2=torch.cat((x2,x3),1)\n",
    "        x4=F.relu(self.conv4(cat2))\n",
    "        x4=self.attn2(x4)\n",
    "        cat3=torch.cat((x1,x2,x3,x4,dwt_in),1)\n",
    "        k=F.relu(self.conv5(cat3))\n",
    "        output=k*x-k+self.b\n",
    "        return F.relu(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1471e",
   "metadata": {
    "papermill": {
     "duration": 0.006222,
     "end_time": "2024-06-29T00:55:40.978056",
     "exception": false,
     "start_time": "2024-06-29T00:55:40.971834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss Definition\n",
    "\n",
    "The loss functions required to train or observe have been defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f9b3cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:40.992569Z",
     "iopub.status.busy": "2024-06-29T00:55:40.992311Z",
     "iopub.status.idle": "2024-06-29T00:55:46.242838Z",
     "shell.execute_reply": "2024-06-29T00:55:46.241781Z"
    },
    "papermill": {
     "duration": 5.260697,
     "end_time": "2024-06-29T00:55:46.245278",
     "exception": false,
     "start_time": "2024-06-29T00:55:40.984581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528M/528M [00:03<00:00, 173MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "class TotalVariationLoss(nn.Module):\n",
    "    def __init__(self,wt=1):\n",
    "        super(TotalVariationLoss,self).__init__()\n",
    "        self.wt=wt\n",
    "    def forward(self,x):\n",
    "        wid_var=torch.sum(torch.pow(x[:,:,:,:-1]-x[:,:,:,1:],2))\n",
    "        ht_var=torch.sum(torch.pow(x[:,:,:-1,:]-x[:,:,1:,:],2))\n",
    "        return self.wt*(ht_var+wid_var)\n",
    "class FFTLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFTLoss,self).__init__()\n",
    "        self.l1_loss=nn.L1Loss()\n",
    "    def forward(self,out,gt):\n",
    "        fft_out=torch.fft.fftn(out,dim=(-2,-1))\n",
    "        fft_gt=torch.fft.fftn(gt,dim=(-2,-1))\n",
    "        amp_out=torch.abs(fft_out)\n",
    "        ph_out=torch.angle(fft_out)\n",
    "        amp_gt=torch.abs(fft_gt)\n",
    "        ph_gt=torch.angle(fft_gt)\n",
    "        amp_loss=self.l1_loss(amp_out,amp_gt)\n",
    "        ph_loss=self.l1_loss(ph_out,ph_gt)\n",
    "        return amp_loss+ph_loss\n",
    "    \n",
    "perceptual_loss=lpips.LPIPS(net='vgg').to(CFG.device)\n",
    "tv_loss=TotalVariationLoss().to(CFG.device)\n",
    "mse_loss=nn.MSELoss().to(CFG.device)\n",
    "fft_loss=FFTLoss().to(CFG.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b306a8b",
   "metadata": {
    "papermill": {
     "duration": 0.010087,
     "end_time": "2024-06-29T00:55:46.266084",
     "exception": false,
     "start_time": "2024-06-29T00:55:46.255997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the Model\n",
    "Here the model is being trained on the image pairs and as an intial step Xavier weight initialisation is being used. The model is being parallelised using the DataParallel function as 2 NVidia T4 GPUs have been used on Kaggle to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669752b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:46.285546Z",
     "iopub.status.busy": "2024-06-29T00:55:46.285272Z",
     "iopub.status.idle": "2024-06-29T00:55:46.305174Z",
     "shell.execute_reply": "2024-06-29T00:55:46.304112Z"
    },
    "papermill": {
     "duration": 0.031889,
     "end_time": "2024-06-29T00:55:46.307103",
     "exception": false,
     "start_time": "2024-06-29T00:55:46.275214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded to GPU..\n"
     ]
    }
   ],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m,torch.nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias,0)\n",
    "dehaze_model=DehazingNet()\n",
    "dehaze_model.apply(weight_init)\n",
    "dehaze_model=nn.DataParallel(dehaze_model)\n",
    "dehaze_model=dehaze_model.to(CFG.device)\n",
    "print(\"Model Loaded to GPU..\")\n",
    "optimizer=optim.Adam(dehaze_model.parameters(),lr=CFG.lr,weight_decay=1e-4)\n",
    "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.1,patience=3,verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0886a281",
   "metadata": {
    "papermill": {
     "duration": 0.009482,
     "end_time": "2024-06-29T00:55:46.344201",
     "exception": false,
     "start_time": "2024-06-29T00:55:46.334719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training Loop\n",
    "The training loop iteratively loops through the training data and trains the model based on the loss functions used. To efficiently optimise the memory usage. Garbage Collection and cache clearence is done in a periodic basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eaba1ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T00:55:46.364152Z",
     "iopub.status.busy": "2024-06-29T00:55:46.363893Z",
     "iopub.status.idle": "2024-06-29T04:45:10.504980Z",
     "shell.execute_reply": "2024-06-29T04:45:10.504062Z"
    },
    "papermill": {
     "duration": 13764.163374,
     "end_time": "2024-06-29T04:45:10.516891",
     "exception": false,
     "start_time": "2024-06-29T00:55:46.353517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 1/14 [19:30<4:13:36, 1170.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà‚ñè       | 3/14 [51:45<3:05:40, 1012.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñä       | 4/14 [1:07:53<2:45:49, 994.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [1:24:01<2:27:49, 985.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience decreased to 4..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [1:40:21<2:11:07, 983.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience decreased to 3..\n",
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [2:12:35<1:37:26, 974.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [2:28:39<1:20:55, 971.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [2:44:46<1:04:40, 970.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience decreased to 4..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11/14 [3:00:51<48:25, 968.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [3:17:08<32:21, 970.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13/14 [3:33:11<16:08, 968.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [3:49:23<00:00, 983.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if CFG.train:\n",
    "    best_val_loss=float('inf')\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    dehaze_model.train()\n",
    "    patience=CFG.es_patience\n",
    "    for epoch in tqdm(range(CFG.epochs)):\n",
    "        total_train_loss=0\n",
    "        total_val_loss=0\n",
    "        for pair in train_loader:\n",
    "            hazy,clear=pair['hazy'].to(CFG.device),pair['gt'].to(CFG.device)\n",
    "            model_out=dehaze_model(hazy)\n",
    "            loss_tv=tv_loss(model_out)\n",
    "            loss_mse=mse_loss(model_out,clear)\n",
    "            loss_perceptual=perceptual_loss(model_out,clear).mean()\n",
    "            loss_fft=fft_loss(model_out,clear)\n",
    "            train_loss=(1/2)*(loss_mse+loss_perceptual)\n",
    "            total_train_loss+=train_loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(dehaze_model.parameters(),1e-2)\n",
    "            optimizer.step()\n",
    "            wandb.log({\"Training FFT Loss\":loss_fft,\"Training TV Loss\":loss_tv,\"Training MSE Loss\":loss_mse,\"Training Perceptual Loss\":loss_perceptual})\n",
    "            del hazy,clear,model_out,loss_tv,loss_mse,loss_fft\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        training_loss=total_train_loss/len(train_loader.dataset)\n",
    "        wandb.log({\"Average Training Loss\":training_loss,\"Epoch\": epoch+1})\n",
    "\n",
    "        dehaze_model.eval()\n",
    "        for val_pair in val_loader:\n",
    "            val_hazy,val_clear=val_pair['hazy'].to(CFG.device),val_pair['gt'].to(CFG.device)\n",
    "            val_output=dehaze_model(val_hazy)\n",
    "            val_loss_mse=mse_loss(val_output,val_clear)\n",
    "            val_loss_tv=tv_loss(val_output)\n",
    "            val_loss_perceptual=perceptual_loss(val_output,val_clear).mean()\n",
    "            val_loss_fft=fft_loss(val_output,val_clear)\n",
    "            val_loss=(1/2)*(val_loss_mse+val_loss_perceptual)\n",
    "            total_val_loss+=val_loss.item()\n",
    "            wandb.log({\"Validation FFT Loss\":val_loss_fft,\"Validation TV Loss\":val_loss_tv,\"Validation MSE Loss\":val_loss_mse,\"Validation Perceptual Loss\":val_loss_perceptual})\n",
    "            del val_hazy,val_clear,val_output,val_loss_mse,val_loss_tv,val_loss_fft,val_loss\n",
    "        validation_loss=total_val_loss/len(val_loader.dataset)\n",
    "        wandb.log({\"Average Validation Loss\":validation_loss})\n",
    "        if (best_val_loss-validation_loss)>1e-4:\n",
    "            best_val_loss=validation_loss\n",
    "            print('Saving Model..')\n",
    "            torch.save(dehaze_model.state_dict(),'dehazer-model-trained-best.pth')    \n",
    "            patience=CFG.es_patience\n",
    "        else:\n",
    "            patience-=1\n",
    "            print(f'Patience decreased to {patience}..')\n",
    "            if patience==0:\n",
    "                print('Early stopping triggered...')\n",
    "                break\n",
    "        \n",
    "        scheduler.step(validation_loss)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.save(dehaze_model.state_dict(),'dehazer-model-trained.pth')\n",
    "  \n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5101434,
     "sourceId": 8656429,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 54085,
     "sourceId": 64839,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13817.599376,
   "end_time": "2024-06-29T04:45:13.387207",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-29T00:54:55.787831",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
