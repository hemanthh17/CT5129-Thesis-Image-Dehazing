{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7462f73",
   "metadata": {
    "papermill": {
     "duration": 0.004872,
     "end_time": "2024-08-03T18:29:44.779097",
     "exception": false,
     "start_time": "2024-08-03T18:29:44.774225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Enhancer CNN Training Data Preparation\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3a1c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:29:44.790512Z",
     "iopub.status.busy": "2024-08-03T18:29:44.790068Z",
     "iopub.status.idle": "2024-08-03T18:30:11.420865Z",
     "shell.execute_reply": "2024-08-03T18:30:11.419709Z"
    },
    "papermill": {
     "duration": 26.639522,
     "end_time": "2024-08-03T18:30:11.423557",
     "exception": false,
     "start_time": "2024-08-03T18:29:44.784035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch-enhance torchmetrics lpips -q\n",
    "import gc,os,cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lpips\n",
    "import pywt\n",
    "import shutil,time\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as tt \n",
    "from torch_enhance.losses import VGG as PerceptualLoss\n",
    "from torchmetrics.image import PeakSignalNoiseRatio,StructuralSimilarityIndexMeasure\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc568e2",
   "metadata": {
    "papermill": {
     "duration": 0.004344,
     "end_time": "2024-08-03T18:30:11.432654",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.428310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa2beab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:30:11.443502Z",
     "iopub.status.busy": "2024-08-03T18:30:11.442948Z",
     "iopub.status.idle": "2024-08-03T18:30:11.449859Z",
     "shell.execute_reply": "2024-08-03T18:30:11.448503Z"
    },
    "papermill": {
     "duration": 0.015089,
     "end_time": "2024-08-03T18:30:11.452200",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.437111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_transforms_rgb=tt.Compose([\n",
    "    tt.transforms.Resize((256,256),antialias=True),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(mean=(0.6344,0.5955,0.5857),std=(0.1742,0.1798,0.1871))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c004422",
   "metadata": {
    "papermill": {
     "duration": 0.004665,
     "end_time": "2024-08-03T18:30:11.461498",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.456833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Initilaizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce0f9d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:30:11.473172Z",
     "iopub.status.busy": "2024-08-03T18:30:11.472788Z",
     "iopub.status.idle": "2024-08-03T18:30:11.503160Z",
     "shell.execute_reply": "2024-08-03T18:30:11.501974Z"
    },
    "papermill": {
     "duration": 0.039101,
     "end_time": "2024-08-03T18:30:11.505403",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.466302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduct_ratio=8):\n",
    "        super(PixelAttention,self).__init__()\n",
    "        reduced_channel=max(1,channel//reduct_ratio)\n",
    "        self.pixel_attention=nn.Sequential(\n",
    "            nn.Conv2d(channel,channel//reduced_channel,kernel_size=1,padding=0,bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel//reduced_channel,1,kernel_size=1,padding=0,bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,feature):\n",
    "        x=self.pixel_attention(feature)\n",
    "        return x*feature\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,input_channels,reduct_ratio=8):\n",
    "        super(ChannelAttention,self).__init__()\n",
    "        reduced_channel=max(1,input_channels//reduct_ratio)\n",
    "        self.avg_pooler=nn.AdaptiveAvgPool2d(1)\n",
    "        self.fcn=nn.Sequential(\n",
    "            nn.Linear(input_channels,reduced_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(reduced_channel,input_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self,input_feature):\n",
    "        n,c,_,_=input_feature.size()\n",
    "        x=self.avg_pooler(input_feature).view(n,c)\n",
    "        x=F.sigmoid(self.fcn(x).view(n,c,1,1))\n",
    "        return input_feature*x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,dims,kernel_size=1):\n",
    "        super(AttentionBlock,self).__init__()\n",
    "        self.conv1=nn.Conv2d(dims,dims,kernel_size,padding=(kernel_size//2),bias=True)\n",
    "        self.conv2=nn.Conv2d(dims,dims,kernel_size,padding=(kernel_size//2),bias=True)\n",
    "        self.ca=ChannelAttention(dims)\n",
    "        self.pa=PixelAttention(dims)\n",
    "    def forward(self,img):\n",
    "        feat=F.relu(self.conv1(img),inplace=True)\n",
    "        feat=feat+img\n",
    "        feat=F.relu(self.conv1(feat),inplace=True)\n",
    "        feat=self.ca(feat)\n",
    "        feat=self.pa(feat)\n",
    "        feat+=img\n",
    "        return feat\n",
    "class DWT_DehazingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DWT_DehazingNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.attn1=AttentionBlock(3)\n",
    "        self.conv3=nn.Conv2d(in_channels=9,out_channels=3,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv4=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=7,stride=1,padding=3)\n",
    "        self.attn2=AttentionBlock(3)\n",
    "        self.conv5=nn.Conv2d(in_channels=15,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_dwt=nn.Conv2d(in_channels=12,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.b=1\n",
    "\n",
    "    def forward(self,x):\n",
    "        dwt_coeffs=pywt.dwt2(x.cpu(),wavelet='db4')\n",
    "        LL,(LH,HL,HH)=dwt_coeffs\n",
    "        dwt_out=torch.concat([torch.from_numpy(LL),torch.from_numpy(LH),torch.from_numpy(HL),torch.from_numpy(HH)],dim=1)\n",
    "        x1=F.relu(self.conv1(x))\n",
    "        dwt_out=tt.Resize((256,256))(dwt_out)\n",
    "        dwt_in=self.conv_dwt(dwt_out)\n",
    "        x2=F.relu(self.conv2(x1))\n",
    "        x2=self.attn1(x2)\n",
    "        cat1=torch.cat((x1,x2,dwt_in),1)\n",
    "        x3=F.relu(self.conv3(cat1))\n",
    "        cat2=torch.cat((x2,x3),1)\n",
    "        x4=F.relu(self.conv4(cat2))\n",
    "        x4=self.attn2(x4)\n",
    "        cat3=torch.cat((x1,x2,x3,x4,dwt_in),1)\n",
    "        k=F.relu(self.conv5(cat3))\n",
    "        output=k*x-k+self.b\n",
    "        return F.relu(output)\n",
    "class DehazingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DehazingNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.attn1=AttentionBlock(3)\n",
    "        self.conv3=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv4=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=7,stride=1,padding=3)\n",
    "        self.attn2=AttentionBlock(3)\n",
    "        self.conv5=nn.Conv2d(in_channels=12,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_dwt=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.b=1\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1=F.relu(self.conv1(x))\n",
    "        x2=F.relu(self.conv2(x1))\n",
    "        x2=self.attn1(x2)\n",
    "        cat1=torch.cat((x1,x2),1)\n",
    "        x3=F.relu(self.conv3(cat1))\n",
    "        cat2=torch.cat((x2,x3),1)\n",
    "        x4=F.relu(self.conv4(cat2))\n",
    "        x4=self.attn2(x4)\n",
    "        cat3=torch.cat((x1,x2,x3,x4),1)\n",
    "        k=F.relu(self.conv5(cat3))\n",
    "        output=k*x-k+self.b\n",
    "        return F.relu(output)\n",
    "class FinalCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FinalCNN,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv3=nn.Conv2d(in_channels=32,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.relu3=nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.relu1(self.conv1(x))\n",
    "        x=self.relu2(self.conv2(x))\n",
    "        x=self.relu3(self.conv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b2ada",
   "metadata": {
    "papermill": {
     "duration": 0.004879,
     "end_time": "2024-08-03T18:30:11.514999",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.510120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d225982",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:30:11.526397Z",
     "iopub.status.busy": "2024-08-03T18:30:11.525987Z",
     "iopub.status.idle": "2024-08-03T18:30:11.598136Z",
     "shell.execute_reply": "2024-08-03T18:30:11.596756Z"
    },
    "papermill": {
     "duration": 0.080906,
     "end_time": "2024-08-03T18:30:11.600933",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.520027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwt_dehazenet_rgb=nn.DataParallel(DWT_DehazingNet())\n",
    "dehazenet_rgb=nn.DataParallel(DehazingNet())\n",
    "final_cnn=FinalCNN()\n",
    "\n",
    "\n",
    "dwt_dehazenet_rgb.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/dehazing-rgb-dwt-2l.pth',map_location=torch.device('cpu')))\n",
    "dehazenet_rgb.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/dehazenet-rgb-2l.pth',map_location=torch.device('cpu')))\n",
    "final_cnn.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/end_cnn.pth',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0ac88",
   "metadata": {
    "papermill": {
     "duration": 0.004574,
     "end_time": "2024-08-03T18:30:11.610524",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.605950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utility Functions for the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb08086a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:30:11.621627Z",
     "iopub.status.busy": "2024-08-03T18:30:11.621233Z",
     "iopub.status.idle": "2024-08-03T18:30:11.639071Z",
     "shell.execute_reply": "2024-08-03T18:30:11.637759Z"
    },
    "papermill": {
     "duration": 0.026342,
     "end_time": "2024-08-03T18:30:11.641592",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.615250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_image_rgb(img_tensor,file_path):\n",
    "    if img_tensor.shape[0]!=3:\n",
    "        img_tensor=img_tensor.squeeze()\n",
    "    rgb_array=tensor_denormalize_rgb(img_tensor).permute(1,2,0).cpu().detach().numpy()\n",
    "    rgb_image=Image.fromarray((np.clip(rgb_array,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    rgb_image.save(file_path)\n",
    "\n",
    "def tensor_denormalize_rgb(out_tensor,mean=[0.4556,0.3837,0.3642],std=[0.2689,0.2691,0.2828]):\n",
    "    if len(out_tensor.shape)==3:\n",
    "        out_tensor=out_tensor.unsqueeze(0)\n",
    "    mean=torch.tensor(mean).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "    std=torch.tensor(std).unsqueeze(0).unsqueeze(2).unsqueeze(3)    \n",
    "    denorm_tensor=(out_tensor*std)+mean\n",
    "    return denorm_tensor.squeeze(0)\n",
    "def unsharp_mask(image,kernel_size=(5,5),sigma=0.4,amount=1.0,threshold=1):\n",
    "    blurred=cv2.GaussianBlur(image,kernel_size,sigma)\n",
    "    sharpened=float(amount+1)*image-float(amount)*blurred\n",
    "    sharpened=np.maximum(sharpened,np.zeros(sharpened.shape))\n",
    "    sharpened=np.minimum(sharpened,255*np.ones(sharpened.shape))\n",
    "    sharpened=sharpened.round().astype(np.uint8)\n",
    "    if threshold>0:\n",
    "        low_contrast_mask=np.absolute(image-blurred)<threshold\n",
    "        np.copyto(sharpened,image,where=low_contrast_mask)\n",
    "    return sharpened\n",
    "\n",
    "def clahe(image):\n",
    "    clahe=cv2.createCLAHE(clipLimit=1,tileGridSize=(2,2))\n",
    "    lab=cv2.cvtColor(image,cv2.COLOR_BGR2LAB)\n",
    "    lab[:,:,0]=clahe.apply(lab[:,:,0])\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def enhance_image(image_path):\n",
    "    image=cv2.imread(image_path)\n",
    "    img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_sharpened=unsharp_mask(img)\n",
    "    image_clahe=clahe(image_sharpened)\n",
    "    image_tensor=tt.ToTensor()(image_clahe)   \n",
    "    return image_tensor\n",
    "def alpha_blending(image1, image2, alpha):\n",
    "    blended = cv2.addWeighted(image1, alpha, image2, 1 - alpha, 4)\n",
    "    return blended\n",
    "def image_addition(coeff,img_path1,img_path2):\n",
    "    img1=cv2.cvtColor(cv2.imread(img_path1),cv2.COLOR_BGR2RGB)\n",
    "    img2=cv2.cvtColor(cv2.imread(img_path2),cv2.COLOR_BGR2RGB)\n",
    "    img_f=tt.ToTensor()(alpha_blending(img1,img2,coeff))    \n",
    "    return img_f\n",
    "def save_image_final(img_tensor,file_path):\n",
    "    if img_tensor.shape[0]!=3:\n",
    "        raise ValueError(\"Input tensor must have 3 channels only...\")\n",
    "    rgb_array=img_tensor.permute(1,2,0).cpu().detach().numpy()\n",
    "    rgb_image=Image.fromarray((np.clip(rgb_array,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    rgb_image.save(file_path)\n",
    "def enhance_image_merged(image_path):\n",
    "    image=cv2.imread(image_path)\n",
    "    img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img2=cv2.fastNlMeansDenoisingColored(img,None,10,10,3,21)\n",
    "    image_sharpened=unsharp_mask(img2)\n",
    "    image_clahe=clahe(image_sharpened)\n",
    "    image_tensor=tt.ToTensor()(image_clahe)   \n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed6cb73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:30:11.653658Z",
     "iopub.status.busy": "2024-08-03T18:30:11.653242Z",
     "iopub.status.idle": "2024-08-03T18:30:11.672425Z",
     "shell.execute_reply": "2024-08-03T18:30:11.671370Z"
    },
    "papermill": {
     "duration": 0.028179,
     "end_time": "2024-08-03T18:30:11.674972",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.646793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "psnr_fn,ssim_fn=PeakSignalNoiseRatio(),StructuralSimilarityIndexMeasure()\n",
    "def metrics_calculator(out_path,clear_path):\n",
    "    out_tensor=tt.ToTensor()(cv2.imread(out_path))\n",
    "    clear_tensor=tt.ToTensor()(cv2.resize(cv2.imread(clear_path),(256,256)))\n",
    "    return psnr_fn(out_tensor,clear_tensor),ssim_fn(out_tensor.unsqueeze(0),clear_tensor.unsqueeze(0))\n",
    "def averager(li):\n",
    "    return sum(li)/len(li)\n",
    "\n",
    "def model1_image_pass(img_tensor,folder_name,i,clear_path):\n",
    "    model_output1=dwt_dehazenet_rgb(img_tensor.unsqueeze(0)).cpu()\n",
    "    save_image_rgb(model_output1.squeeze(),f'/kaggle/working/{folder_name}/output_image_{i}_1.png')\n",
    "    psnr1_1,ssim1_1=metrics_calculator(f'/kaggle/working/{folder_name}/output_image_{i}_1.png',clear_path)\n",
    "    proc_out_tensor1=enhance_image(f'/kaggle/working/{folder_name}/output_image_{i}_1.png')\n",
    "    proc_arr1=proc_out_tensor1.permute(1,2,0).cpu().detach().numpy()\n",
    "    proc_image1=Image.fromarray((np.clip(proc_arr1,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    proc_image1.save(f'/kaggle/working/{folder_name}/processed_image_{i}_1.png')\n",
    "    psnr2_1,ssim_2_1=metrics_calculator(f'/kaggle/working/{folder_name}/processed_image_{i}_1.png',clear_path)\n",
    "    return proc_arr1,psnr1_1,ssim1_1,psnr2_1,ssim_2_1\n",
    "    \n",
    "    \n",
    "def model2_image_pass(img_tensor,folder_name,i,clear_path):\n",
    "    model_output2=dehazenet_rgb(img_tensor.unsqueeze(0)).cpu()\n",
    "    save_image_rgb(model_output2.squeeze(),f'/kaggle/working/{folder_name}/output_image_{i}_2.png')\n",
    "    psnr1_2,ssim1_2=metrics_calculator(f'/kaggle/working/{folder_name}/output_image_{i}_2.png',clear_path)\n",
    "    proc_out_tensor2=enhance_image(f'/kaggle/working/{folder_name}/output_image_{i}_2.png')\n",
    "    proc_arr2=proc_out_tensor2.permute(1,2,0).cpu().detach().numpy()\n",
    "    proc_image2=Image.fromarray((np.clip(proc_arr2,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    proc_image2.save(f'/kaggle/working/{folder_name}/processed_image_{i}_2.png')\n",
    "    psnr2_2,ssim_2_2=metrics_calculator(f'/kaggle/working/{folder_name}/processed_image_{i}_2.png',clear_path)\n",
    "    psnr,ssim=metrics_calculator(f'/kaggle/working/{folder_name}/processed_image_{i}_2.png',clear_path)\n",
    "    return proc_arr2,psnr1_2,ssim1_2,psnr2_2,ssim_2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb16e0",
   "metadata": {
    "papermill": {
     "duration": 0.004352,
     "end_time": "2024-08-03T18:30:11.684097",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.679745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Complete Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73a4b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:30:11.695842Z",
     "iopub.status.busy": "2024-08-03T18:30:11.695414Z",
     "iopub.status.idle": "2024-08-03T18:30:11.707977Z",
     "shell.execute_reply": "2024-08-03T18:30:11.706457Z"
    },
    "papermill": {
     "duration": 0.021914,
     "end_time": "2024-08-03T18:30:11.710714",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.688800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_run_parallel(haze_img_path,clear_img_path,coeff=0.6,i=0,folder_name='Inference_Results'):\n",
    "    os.makedirs(f'/kaggle/working/{folder_name}',exist_ok=True)\n",
    "    os.makedirs(f'/kaggle/working/{folder_name}_blended',exist_ok=True)\n",
    "    dwt_dehazenet_rgb.eval()\n",
    "    dwt_dehazenet_rgb.eval()\n",
    "    start_time=time.time()\n",
    "    haze_img_tensor=cv2.cvtColor(cv2.imread(haze_img_path),cv2.COLOR_BGR2RGB)\n",
    "    haze_img_tensor=Image.open(haze_img_path)\n",
    "    img_tensor=input_transforms_rgb(haze_img_tensor).cpu()\n",
    "    out1,out2=Parallel(n_jobs=2)(delayed(func)(img_tensor,folder_name,i,clear_img_path) for func in [model1_image_pass,model2_image_pass])\n",
    "    #out1,out2=model1_image_pass(img_tensor,folder_name,i,clear_img_path),model2_image_pass(img_tensor,folder_name,i,clear_img_path)\n",
    "    psnr_mod11,ssim_mod11,psnr_mod21,ssim_mod21,psnr_mod12,ssim_mod12,psnr_mod22,ssim_mod22=out1[1],out1[2],out1[3],out1[4],out2[1],out2[2],out2[3],out2[4]\n",
    "    blended_image=image_addition(coeff,f'/kaggle/working/{folder_name}/processed_image_{i}_2.png',\n",
    "                                 f'/kaggle/working/{folder_name}/processed_image_{i}_1.png')\n",
    "    save_image_final(blended_image.squeeze(),f'/kaggle/working/{folder_name}/merged_image_{i}.png')\n",
    "    psnr_blend,ssim_blend=metrics_calculator(f'/kaggle/working/{folder_name}/merged_image_{i}.png',clear_img_path)\n",
    "    save_image_rgb(tt.ToTensor()(cv2.cvtColor(cv2.imread(clear_img_path),cv2.COLOR_BGR2RGB)),f'/kaggle/working/{folder_name}_blended/clear_image_{i}.png')\n",
    "    proc_blended=enhance_image_merged(f'/kaggle/working/{folder_name}/merged_image_{i}.png')\n",
    "    save_image_final(proc_blended.squeeze(),f'/kaggle/working/{folder_name}_blended/proc_merged_image_{i}.png')\n",
    "    psnr_procblend,ssim_procblend=metrics_calculator(f'/kaggle/working/{folder_name}_blended/proc_merged_image_{i}.png',clear_img_path)\n",
    "    return psnr_mod11,ssim_mod11,psnr_mod21,ssim_mod21,psnr_mod12,ssim_mod12,psnr_mod22,ssim_mod22,psnr_blend,ssim_blend,psnr_procblend,ssim_procblend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1311564a",
   "metadata": {
    "papermill": {
     "duration": 0.00448,
     "end_time": "2024-08-03T18:30:11.719973",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.715493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preparing BeDDE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b48ad9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:30:11.731462Z",
     "iopub.status.busy": "2024-08-03T18:30:11.731069Z",
     "iopub.status.idle": "2024-08-03T18:30:11.799185Z",
     "shell.execute_reply": "2024-08-03T18:30:11.798055Z"
    },
    "papermill": {
     "duration": 0.077243,
     "end_time": "2024-08-03T18:30:11.802008",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.724765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dataset=pd.read_csv('/kaggle/input/dehazing-dataset-thesis/dehaing_dataset_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad7fe8",
   "metadata": {
    "papermill": {
     "duration": 0.004525,
     "end_time": "2024-08-03T18:30:11.811886",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.807361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Procuring Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da8f11fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:30:11.823738Z",
     "iopub.status.busy": "2024-08-03T18:30:11.823322Z",
     "iopub.status.idle": "2024-08-03T22:27:13.280051Z",
     "shell.execute_reply": "2024-08-03T22:27:13.278440Z"
    },
    "papermill": {
     "duration": 14221.468866,
     "end_time": "2024-08-03T22:27:13.285922",
     "exception": false,
     "start_time": "2024-08-03T18:30:11.817056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed....\n",
      "Average workflow processing time: 1.0060368128534674\n",
      "Average metrics at DWT Dehazernet: 14.0844144821167 and 0.6564133763313293\n",
      "Average metrics at Postprocessing DWT Dehazernet: 15.833892822265625 and 0.7221300005912781\n",
      "Average metrics at Dehazernet: 14.057572364807129 and 0.6625691056251526\n",
      "Average metrics at Postprocessing Dehazernet: 15.794137954711914 and 0.7292566895484924\n",
      "Average metrics at Blending: 15.523079872131348 and 0.7259417176246643\n",
      "Average metrics at Postprocessing Blending: 15.993538856506348 and 0.6529923677444458\n"
     ]
    }
   ],
   "source": [
    "time_data,psnr_mod11,ssim_mod11,psnr_mod21,ssim_mod21,psnr_mod12,ssim_mod12,psnr_mod22,ssim_mod22,psnr_blend,ssim_blend,psnr_procblend,ssim_procblend,psnr_cnn,ssim_cnn=[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "for idx,(clear,hazy) in enumerate(training_dataset.values):\n",
    "    coeff=0.6\n",
    "    start_time=time.time()\n",
    "    outputs=model_run_parallel(hazy,clear,coeff,idx)\n",
    "    time_data.append(time.time()-start_time)\n",
    "    psnr_mod11.append(outputs[0])\n",
    "    ssim_mod11.append(outputs[1])\n",
    "    psnr_mod21.append(outputs[2])\n",
    "    ssim_mod21.append(outputs[3])\n",
    "    psnr_mod12.append(outputs[4])\n",
    "    ssim_mod12.append(outputs[5])\n",
    "    psnr_mod22.append(outputs[6])\n",
    "    ssim_mod22.append(outputs[7])\n",
    "    psnr_blend.append(outputs[8])\n",
    "    ssim_blend.append(outputs[9])\n",
    "    psnr_procblend.append(outputs[10])\n",
    "    ssim_procblend.append(outputs[11])\n",
    "        \n",
    "\n",
    "print('Processed....')\n",
    "print(f\"Average workflow processing time: {averager(time_data)}\")\n",
    "print(f\"Average metrics at DWT Dehazernet: {averager(psnr_mod11)} and {averager(ssim_mod11)}\")\n",
    "print(f\"Average metrics at Postprocessing DWT Dehazernet: {averager(psnr_mod21)} and {averager(ssim_mod21)}\")\n",
    "print(f\"Average metrics at Dehazernet: {averager(psnr_mod12)} and {averager(ssim_mod12)}\")\n",
    "print(f\"Average metrics at Postprocessing Dehazernet: {averager(psnr_mod22)} and {averager(ssim_mod22)}\")\n",
    "print(f\"Average metrics at Blending: {averager(psnr_blend)} and {averager(ssim_blend)}\")\n",
    "print(f\"Average metrics at Postprocessing Blending: {averager(psnr_procblend)} and {averager(ssim_procblend)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18cbc901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T22:27:13.301898Z",
     "iopub.status.busy": "2024-08-03T22:27:13.300731Z",
     "iopub.status.idle": "2024-08-03T22:31:54.740684Z",
     "shell.execute_reply": "2024-08-03T22:31:54.739280Z"
    },
    "papermill": {
     "duration": 281.458777,
     "end_time": "2024-08-03T22:31:54.750565",
     "exception": false,
     "start_time": "2024-08-03T22:27:13.291788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/Inference_Results_blended.zip'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "folder_name = 'Inference_Results'\n",
    "base_path = f'/kaggle/working/{folder_name}_blended'\n",
    "shutil.make_archive(base_path, 'zip', base_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5101434,
     "sourceId": 8729607,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5239863,
     "sourceId": 8857586,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14535.922887,
   "end_time": "2024-08-03T22:31:57.627406",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-03T18:29:41.704519",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
