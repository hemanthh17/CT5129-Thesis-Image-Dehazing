{
 "cells": [
  {
   "cell_type": "raw",
   "id": "66dd9f54",
   "metadata": {
    "papermill": {
     "duration": 0.00383,
     "end_time": "2024-08-04T13:55:36.840607",
     "exception": false,
     "start_time": "2024-08-04T13:55:36.836777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference Script\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1dc0b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:55:36.849288Z",
     "iopub.status.busy": "2024-08-04T13:55:36.848875Z",
     "iopub.status.idle": "2024-08-04T13:55:44.565935Z",
     "shell.execute_reply": "2024-08-04T13:55:44.564560Z"
    },
    "papermill": {
     "duration": 7.72451,
     "end_time": "2024-08-04T13:55:44.568742",
     "exception": false,
     "start_time": "2024-08-04T13:55:36.844232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc,os,cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pywt\n",
    "import shutil,time\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as tt \n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704ff1fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:55:44.577981Z",
     "iopub.status.busy": "2024-08-04T13:55:44.577180Z",
     "iopub.status.idle": "2024-08-04T13:55:44.583647Z",
     "shell.execute_reply": "2024-08-04T13:55:44.582464Z"
    },
    "papermill": {
     "duration": 0.013673,
     "end_time": "2024-08-04T13:55:44.586051",
     "exception": false,
     "start_time": "2024-08-04T13:55:44.572378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_transforms_rgb=tt.Compose([\n",
    "    tt.transforms.Resize((256,256),antialias=True),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(mean=(0.6344,0.5955,0.5857),std=(0.1742,0.1798,0.1871))\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1376e07",
   "metadata": {
    "papermill": {
     "duration": 0.003288,
     "end_time": "2024-08-04T13:55:44.592921",
     "exception": false,
     "start_time": "2024-08-04T13:55:44.589633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Initilaizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb62963d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:55:44.602176Z",
     "iopub.status.busy": "2024-08-04T13:55:44.601460Z",
     "iopub.status.idle": "2024-08-04T13:55:44.633538Z",
     "shell.execute_reply": "2024-08-04T13:55:44.632504Z"
    },
    "papermill": {
     "duration": 0.039843,
     "end_time": "2024-08-04T13:55:44.636156",
     "exception": false,
     "start_time": "2024-08-04T13:55:44.596313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduct_ratio=8):\n",
    "        super(PixelAttention,self).__init__()\n",
    "        reduced_channel=max(1,channel//reduct_ratio)\n",
    "        self.pixel_attention=nn.Sequential(\n",
    "            nn.Conv2d(channel,channel//reduced_channel,kernel_size=1,padding=0,bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel//reduced_channel,1,kernel_size=1,padding=0,bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,feature):\n",
    "        x=self.pixel_attention(feature)\n",
    "        return x*feature\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,input_channels,reduct_ratio=8):\n",
    "        super(ChannelAttention,self).__init__()\n",
    "        reduced_channel=max(1,input_channels//reduct_ratio)\n",
    "        self.avg_pooler=nn.AdaptiveAvgPool2d(1)\n",
    "        self.fcn=nn.Sequential(\n",
    "            nn.Linear(input_channels,reduced_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(reduced_channel,input_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self,input_feature):\n",
    "        n,c,_,_=input_feature.size()\n",
    "        x=self.avg_pooler(input_feature).view(n,c)\n",
    "        x=F.sigmoid(self.fcn(x).view(n,c,1,1))\n",
    "        return input_feature*x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,dims,kernel_size=1):\n",
    "        super(AttentionBlock,self).__init__()\n",
    "        self.conv1=nn.Conv2d(dims,dims,kernel_size,padding=(kernel_size//2),bias=True)\n",
    "        self.conv2=nn.Conv2d(dims,dims,kernel_size,padding=(kernel_size//2),bias=True)\n",
    "        self.ca=ChannelAttention(dims)\n",
    "        self.pa=PixelAttention(dims)\n",
    "    def forward(self,img):\n",
    "        feat=F.relu(self.conv1(img),inplace=True)\n",
    "        feat=feat+img\n",
    "        feat=F.relu(self.conv1(feat),inplace=True)\n",
    "        feat=self.ca(feat)\n",
    "        feat=self.pa(feat)\n",
    "        feat+=img\n",
    "        return feat\n",
    "class DWT_DehazingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DWT_DehazingNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.attn1=AttentionBlock(3)\n",
    "        self.conv3=nn.Conv2d(in_channels=9,out_channels=3,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv4=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=7,stride=1,padding=3)\n",
    "        self.attn2=AttentionBlock(3)\n",
    "        self.conv5=nn.Conv2d(in_channels=15,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_dwt=nn.Conv2d(in_channels=12,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.b=1\n",
    "\n",
    "    def forward(self,x):\n",
    "        dwt_coeffs=pywt.dwt2(x.cpu(),wavelet='db4')\n",
    "        LL,(LH,HL,HH)=dwt_coeffs\n",
    "        dwt_out=torch.concat([torch.from_numpy(LL),torch.from_numpy(LH),torch.from_numpy(HL),torch.from_numpy(HH)],dim=1)\n",
    "        x1=F.relu(self.conv1(x))\n",
    "        dwt_out=tt.Resize((256,256))(dwt_out)\n",
    "        dwt_in=self.conv_dwt(dwt_out)\n",
    "        x2=F.relu(self.conv2(x1))\n",
    "        x2=self.attn1(x2)\n",
    "        cat1=torch.cat((x1,x2,dwt_in),1)\n",
    "        x3=F.relu(self.conv3(cat1))\n",
    "        cat2=torch.cat((x2,x3),1)\n",
    "        x4=F.relu(self.conv4(cat2))\n",
    "        x4=self.attn2(x4)\n",
    "        cat3=torch.cat((x1,x2,x3,x4,dwt_in),1)\n",
    "        k=F.relu(self.conv5(cat3))\n",
    "        return F.relu(k*x-k+self.b)\n",
    "class DehazingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DehazingNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.attn1=AttentionBlock(3)\n",
    "        self.conv3=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv4=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=7,stride=1,padding=3)\n",
    "        self.attn2=AttentionBlock(3)\n",
    "        self.conv5=nn.Conv2d(in_channels=12,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_dwt=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.b=1\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1=F.relu(self.conv1(x))\n",
    "        x2=F.relu(self.conv2(x1))\n",
    "        x2=self.attn1(x2)\n",
    "        cat1=torch.cat((x1,x2),1)\n",
    "        x3=F.relu(self.conv3(cat1))\n",
    "        cat2=torch.cat((x2,x3),1)\n",
    "        x4=F.relu(self.conv4(cat2))\n",
    "        x4=self.attn2(x4)\n",
    "        cat3=torch.cat((x1,x2,x3,x4),1)\n",
    "        k=F.relu(self.conv5(cat3))\n",
    "        return F.relu(k*x-k+self.b)\n",
    "class FinalCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FinalCNN, self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv3=nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv4=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv5=nn.Conv2d(in_channels=64,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv6=nn.Conv2d(in_channels=32,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv7=nn.Conv2d(in_channels=16,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1=self.relu(self.conv1(x))\n",
    "        x2=self.relu(self.conv2(x1))\n",
    "        x3=self.relu(self.conv3(x2))\n",
    "        x4=self.relu(self.conv4(x3))\n",
    "        x5=self.relu(self.conv5(x4))+x3\n",
    "        x6=self.relu(self.conv6(x5))+x1\n",
    "        x7=self.relu(self.conv7(x6)) \n",
    "        return self.relu(x7*x-x7+1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ccb67d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:55:44.645294Z",
     "iopub.status.busy": "2024-08-04T13:55:44.644940Z",
     "iopub.status.idle": "2024-08-04T13:55:44.731396Z",
     "shell.execute_reply": "2024-08-04T13:55:44.730301Z"
    },
    "papermill": {
     "duration": 0.093574,
     "end_time": "2024-08-04T13:55:44.733772",
     "exception": false,
     "start_time": "2024-08-04T13:55:44.640198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwt_dehazenet_rgb=nn.DataParallel(DWT_DehazingNet())\n",
    "dehazenet_rgb=nn.DataParallel(DehazingNet())\n",
    "final_cnn=nn.DataParallel(FinalCNN())\n",
    "\n",
    "\n",
    "dwt_dehazenet_rgb.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/dehazing-rgb-dwt-2l.pth',map_location=torch.device('cpu')))\n",
    "dehazenet_rgb.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/dehazenet-rgb-2l.pth',map_location=torch.device('cpu')))\n",
    "final_cnn.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/end_cnn.pth',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ebc5b89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:55:44.742875Z",
     "iopub.status.busy": "2024-08-04T13:55:44.742513Z",
     "iopub.status.idle": "2024-08-04T13:55:44.759731Z",
     "shell.execute_reply": "2024-08-04T13:55:44.758691Z"
    },
    "papermill": {
     "duration": 0.024441,
     "end_time": "2024-08-04T13:55:44.762018",
     "exception": false,
     "start_time": "2024-08-04T13:55:44.737577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_image_rgb(img_tensor,file_path):\n",
    "    if img_tensor.shape[0]!=3:\n",
    "        raise ValueError(\"Input tensor must have 3 channels only...\")\n",
    "    rgb_array=tensor_denormalize_rgb(img_tensor).permute(1,2,0).cpu().detach().numpy()\n",
    "    rgb_image=Image.fromarray((np.clip(rgb_array,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    rgb_image.save(file_path)\n",
    "\n",
    "def tensor_denormalize_rgb(out_tensor,mean=[0.4556,0.3837,0.3642],std=[0.2689,0.2691,0.2828]):\n",
    "    if len(out_tensor.shape)==3:\n",
    "        out_tensor=out_tensor.unsqueeze(0)\n",
    "    mean=torch.tensor(mean).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "    std=torch.tensor(std).unsqueeze(0).unsqueeze(2).unsqueeze(3)    \n",
    "    denorm_tensor=(out_tensor*std)+mean\n",
    "    return denorm_tensor.squeeze(0)\n",
    "def unsharp_mask(image,kernel_size=(5,5),sigma=0.4,amount=1.0,threshold=1):\n",
    "    blurred=cv2.GaussianBlur(image,kernel_size,sigma)\n",
    "    sharpened=float(amount+1)*image-float(amount)*blurred\n",
    "    sharpened=np.maximum(sharpened,np.zeros(sharpened.shape))\n",
    "    sharpened=np.minimum(sharpened,255*np.ones(sharpened.shape))\n",
    "    sharpened=sharpened.round().astype(np.uint8)\n",
    "    if threshold>0:\n",
    "        low_contrast_mask=np.absolute(image-blurred)<threshold\n",
    "        np.copyto(sharpened,image,where=low_contrast_mask)\n",
    "    return sharpened\n",
    "\n",
    "def clahe(image):\n",
    "    clahe=cv2.createCLAHE(clipLimit=1,tileGridSize=(2,2))\n",
    "    lab=cv2.cvtColor(image,cv2.COLOR_BGR2LAB)\n",
    "    lab[:,:,0]=clahe.apply(lab[:,:,0])\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def enhance_image(image_path):\n",
    "    image=cv2.imread(image_path)\n",
    "    img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_sharpened=unsharp_mask(img)\n",
    "    image_clahe=clahe(image_sharpened)\n",
    "    image_tensor=tt.ToTensor()(image_clahe)   \n",
    "    return image_tensor\n",
    "def alpha_blending(image1, image2, alpha=0.6):\n",
    "    blended = cv2.addWeighted(image1, alpha, image2, 1 - alpha, 4)\n",
    "    return blended\n",
    "def image_addition(coeff,img_path1,img_path2):\n",
    "    img1=cv2.cvtColor(cv2.imread(img_path1),cv2.COLOR_BGR2RGB)\n",
    "    img2=cv2.cvtColor(cv2.imread(img_path2),cv2.COLOR_BGR2RGB)\n",
    "    img_f=tt.ToTensor()(alpha_blending(img1,img2))    \n",
    "    return img_f\n",
    "def save_image_final(img_tensor,file_path):\n",
    "    if img_tensor.shape[0]!=3:\n",
    "        raise ValueError(\"Input tensor must have 3 channels only...\")\n",
    "    rgb_array=img_tensor.permute(1,2,0).cpu().detach().numpy()\n",
    "    rgb_image=Image.fromarray((np.clip(rgb_array,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    rgb_image.save(file_path)\n",
    "def enhance_image_merged(image_path):\n",
    "    image=cv2.imread(image_path)\n",
    "    img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img2=cv2.fastNlMeansDenoisingColored(img,None,10,10,3,21)\n",
    "    image_sharpened=unsharp_mask(img2)\n",
    "    image_clahe=clahe(image_sharpened)\n",
    "    image_tensor=tt.ToTensor()(image_clahe)   \n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd8cd14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:55:44.771344Z",
     "iopub.status.busy": "2024-08-04T13:55:44.770561Z",
     "iopub.status.idle": "2024-08-04T13:55:44.779766Z",
     "shell.execute_reply": "2024-08-04T13:55:44.778539Z"
    },
    "papermill": {
     "duration": 0.016373,
     "end_time": "2024-08-04T13:55:44.782066",
     "exception": false,
     "start_time": "2024-08-04T13:55:44.765693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model1_image_pass(img_tensor,folder_name,i):\n",
    "    model_output1=dwt_dehazenet_rgb(img_tensor.unsqueeze(0)).cpu()\n",
    "    save_image_rgb(model_output1.squeeze(),f'/kaggle/working/{folder_name}/output_image_{i}_1.png')\n",
    "    proc_out_tensor1=enhance_image(f'/kaggle/working/{folder_name}/output_image_{i}_1.png')\n",
    "    proc_arr1=proc_out_tensor1.permute(1,2,0).cpu().detach().numpy()\n",
    "    proc_image1=Image.fromarray((np.clip(proc_arr1,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    proc_image1.save(f'/kaggle/working/{folder_name}/processed_image_{i}_1.png')\n",
    "    return proc_arr1\n",
    "    \n",
    "    \n",
    "def model2_image_pass(img_tensor,folder_name,i):\n",
    "    model_output2=dehazenet_rgb(img_tensor.unsqueeze(0)).cpu()\n",
    "    save_image_rgb(model_output2.squeeze(),f'/kaggle/working/{folder_name}/output_image_{i}_2.png')\n",
    "    proc_out_tensor2=enhance_image(f'/kaggle/working/{folder_name}/output_image_{i}_2.png')\n",
    "    proc_arr2=proc_out_tensor2.permute(1,2,0).cpu().detach().numpy()\n",
    "    proc_image2=Image.fromarray((np.clip(proc_arr2,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    proc_image2.save(f'/kaggle/working/{folder_name}/processed_image_{i}_2.png')\n",
    "    return proc_arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c5adb5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:55:44.791030Z",
     "iopub.status.busy": "2024-08-04T13:55:44.790681Z",
     "iopub.status.idle": "2024-08-04T13:55:49.624792Z",
     "shell.execute_reply": "2024-08-04T13:55:49.623572Z"
    },
    "papermill": {
     "duration": 4.841459,
     "end_time": "2024-08-04T13:55:49.627307",
     "exception": false,
     "start_time": "2024-08-04T13:55:44.785848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def model_run_parallel(haze_img_path,i=0,folder_name='Inference_Results'):\n",
    "    os.makedirs(f'/kaggle/working/{folder_name}',exist_ok=True)\n",
    "    dwt_dehazenet_rgb.eval()\n",
    "    dwt_dehazenet_rgb.eval()\n",
    "    start_time=time.time()\n",
    "    haze_img_tensor=cv2.cvtColor(cv2.imread(haze_img_path),cv2.COLOR_BGR2RGB)\n",
    "    haze_img_tensor=Image.open(haze_img_path)\n",
    "    img_tensor=input_transforms_rgb(haze_img_tensor).cpu()\n",
    "    \n",
    "    arr1,arr2=Parallel(n_jobs=2)(delayed(func)(img_tensor,folder_name,i) for func in [model1_image_pass,model2_image_pass])\n",
    "    blended_image=image_addition(0.6,f'/kaggle/working/{folder_name}/processed_image_{i}_2.png',\n",
    "                                 f'/kaggle/working/{folder_name}/processed_image_{i}_1.png')\n",
    "    save_image_final(blended_image.squeeze(),f'/kaggle/working/{folder_name}/merged_image_{i}.png')\n",
    "    proc_blended=enhance_image_merged(f'/kaggle/working/{folder_name}/merged_image_{i}.png')\n",
    "    save_image_final(blended_image.squeeze(),f'/kaggle/working/{folder_name}/proc_merged_image_{i}.png')\n",
    "    cnn_processed=final_cnn(tt.ToTensor()(cv2.cvtColor(cv2.imread(f'/kaggle/working/{folder_name}/proc_merged_image_{i}.png'),\n",
    "                                                       cv2.COLOR_BGR2RGB)))\n",
    "    save_image_final(cnn_processed.squeeze(),f'/kaggle/working/{folder_name}/cnn_processed_image_{i}.png')\n",
    "\n",
    "model_run_parallel('/kaggle/input/dehazing-dataset-thesis/NH-HAZE/NH-HAZE/01_hazy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88aed96a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:55:49.637167Z",
     "iopub.status.busy": "2024-08-04T13:55:49.636739Z",
     "iopub.status.idle": "2024-08-04T14:01:41.916583Z",
     "shell.execute_reply": "2024-08-04T14:01:41.915477Z"
    },
    "papermill": {
     "duration": 352.290678,
     "end_time": "2024-08-04T14:01:41.922021",
     "exception": false,
     "start_time": "2024-08-04T13:55:49.631343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed....\n",
      "Average workflow processing time: 0.6773859395430638\n"
     ]
    }
   ],
   "source": [
    "test_data=pd.read_csv('/kaggle/input/dehazing-dataset-thesis/dehazing_dataset_test.csv')\n",
    "time_data=[]\n",
    "for idx,(path) in enumerate(test_data.Hazy.values):\n",
    "    start_time=time.time()\n",
    "    model_run_parallel(path,idx)\n",
    "    time_data.append(time.time()-start_time)\n",
    "print('Processed....')\n",
    "print(f\"Average workflow processing time: {sum(time_data)/len(time_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968add09",
   "metadata": {
    "papermill": {
     "duration": 0.003525,
     "end_time": "2024-08-04T14:01:41.929361",
     "exception": false,
     "start_time": "2024-08-04T14:01:41.925836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8883664,
     "datasetId": 5101434,
     "sourceId": 8729607,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 9278373,
     "datasetId": 5239863,
     "sourceId": 9103094,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 370.532478,
   "end_time": "2024-08-04T14:01:44.555749",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-04T13:55:34.023271",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
