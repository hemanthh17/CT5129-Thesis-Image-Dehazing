{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d161a7",
   "metadata": {
    "papermill": {
     "duration": 0.00656,
     "end_time": "2024-08-04T13:37:09.239282",
     "exception": false,
     "start_time": "2024-08-04T13:37:09.232722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BeDDE Dataset Inference \n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958f47ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:37:09.254349Z",
     "iopub.status.busy": "2024-08-04T13:37:09.253868Z",
     "iopub.status.idle": "2024-08-04T13:37:39.567523Z",
     "shell.execute_reply": "2024-08-04T13:37:39.566304Z"
    },
    "papermill": {
     "duration": 30.324323,
     "end_time": "2024-08-04T13:37:39.570401",
     "exception": false,
     "start_time": "2024-08-04T13:37:09.246078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch-enhance torchmetrics lpips -q\n",
    "import gc,os,cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lpips\n",
    "import pywt\n",
    "import shutil,time\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as tt \n",
    "from torch_enhance.losses import VGG as PerceptualLoss\n",
    "from torchmetrics.image import PeakSignalNoiseRatio,StructuralSimilarityIndexMeasure\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17530ce3",
   "metadata": {
    "papermill": {
     "duration": 0.006725,
     "end_time": "2024-08-04T13:37:39.583701",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.576976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "675133fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:37:39.597607Z",
     "iopub.status.busy": "2024-08-04T13:37:39.596994Z",
     "iopub.status.idle": "2024-08-04T13:37:39.604121Z",
     "shell.execute_reply": "2024-08-04T13:37:39.602659Z"
    },
    "papermill": {
     "duration": 0.017035,
     "end_time": "2024-08-04T13:37:39.606710",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.589675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_transforms_rgb=tt.Compose([\n",
    "    tt.transforms.Resize((256,256),antialias=True),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(mean=(0.6344,0.5955,0.5857),std=(0.1742,0.1798,0.1871))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e04c5",
   "metadata": {
    "papermill": {
     "duration": 0.005838,
     "end_time": "2024-08-04T13:37:39.618655",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.612817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Initilaizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072e8cce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:37:39.633053Z",
     "iopub.status.busy": "2024-08-04T13:37:39.632608Z",
     "iopub.status.idle": "2024-08-04T13:37:39.677160Z",
     "shell.execute_reply": "2024-08-04T13:37:39.675891Z"
    },
    "papermill": {
     "duration": 0.055056,
     "end_time": "2024-08-04T13:37:39.679889",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.624833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduct_ratio=8):\n",
    "        super(PixelAttention,self).__init__()\n",
    "        reduced_channel=max(1,channel//reduct_ratio)\n",
    "        self.pixel_attention=nn.Sequential(\n",
    "            nn.Conv2d(channel,channel//reduced_channel,kernel_size=1,padding=0,bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel//reduced_channel,1,kernel_size=1,padding=0,bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,feature):\n",
    "        x=self.pixel_attention(feature)\n",
    "        return x*feature\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,input_channels,reduct_ratio=8):\n",
    "        super(ChannelAttention,self).__init__()\n",
    "        reduced_channel=max(1,input_channels//reduct_ratio)\n",
    "        self.avg_pooler=nn.AdaptiveAvgPool2d(1)\n",
    "        self.fcn=nn.Sequential(\n",
    "            nn.Linear(input_channels,reduced_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(reduced_channel,input_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self,input_feature):\n",
    "        n,c,_,_=input_feature.size()\n",
    "        x=self.avg_pooler(input_feature).view(n,c)\n",
    "        x=F.sigmoid(self.fcn(x).view(n,c,1,1))\n",
    "        return input_feature*x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,dims,kernel_size=1):\n",
    "        super(AttentionBlock,self).__init__()\n",
    "        self.conv1=nn.Conv2d(dims,dims,kernel_size,padding=(kernel_size//2),bias=True)\n",
    "        self.conv2=nn.Conv2d(dims,dims,kernel_size,padding=(kernel_size//2),bias=True)\n",
    "        self.ca=ChannelAttention(dims)\n",
    "        self.pa=PixelAttention(dims)\n",
    "    def forward(self,img):\n",
    "        feat=F.relu(self.conv1(img),inplace=True)\n",
    "        feat=feat+img\n",
    "        feat=F.relu(self.conv1(feat),inplace=True)\n",
    "        feat=self.ca(feat)\n",
    "        feat=self.pa(feat)\n",
    "        feat+=img\n",
    "        return feat\n",
    "class DWT_DehazingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DWT_DehazingNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.attn1=AttentionBlock(3)\n",
    "        self.conv3=nn.Conv2d(in_channels=9,out_channels=3,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv4=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=7,stride=1,padding=3)\n",
    "        self.attn2=AttentionBlock(3)\n",
    "        self.conv5=nn.Conv2d(in_channels=15,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_dwt=nn.Conv2d(in_channels=12,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.b=1\n",
    "\n",
    "    def forward(self,x):\n",
    "        dwt_coeffs=pywt.dwt2(x.cpu(),wavelet='db4')\n",
    "        LL,(LH,HL,HH)=dwt_coeffs\n",
    "        dwt_out=torch.concat([torch.from_numpy(LL),torch.from_numpy(LH),torch.from_numpy(HL),torch.from_numpy(HH)],dim=1)\n",
    "        x1=F.relu(self.conv1(x))\n",
    "        dwt_out=tt.Resize((256,256))(dwt_out)\n",
    "        dwt_in=self.conv_dwt(dwt_out)\n",
    "        x2=F.relu(self.conv2(x1))\n",
    "        x2=self.attn1(x2)\n",
    "        cat1=torch.cat((x1,x2,dwt_in),1)\n",
    "        x3=F.relu(self.conv3(cat1))\n",
    "        cat2=torch.cat((x2,x3),1)\n",
    "        x4=F.relu(self.conv4(cat2))\n",
    "        x4=self.attn2(x4)\n",
    "        cat3=torch.cat((x1,x2,x3,x4,dwt_in),1)\n",
    "        k=F.relu(self.conv5(cat3))\n",
    "        output=k*x-k+self.b\n",
    "        return F.relu(output)\n",
    "class DehazingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DehazingNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2=nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.attn1=AttentionBlock(3)\n",
    "        self.conv3=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv4=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=7,stride=1,padding=3)\n",
    "        self.attn2=AttentionBlock(3)\n",
    "        self.conv5=nn.Conv2d(in_channels=12,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_dwt=nn.Conv2d(in_channels=6,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.b=1\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1=F.relu(self.conv1(x))\n",
    "        x2=F.relu(self.conv2(x1))\n",
    "        x2=self.attn1(x2)\n",
    "        cat1=torch.cat((x1,x2),1)\n",
    "        x3=F.relu(self.conv3(cat1))\n",
    "        cat2=torch.cat((x2,x3),1)\n",
    "        x4=F.relu(self.conv4(cat2))\n",
    "        x4=self.attn2(x4)\n",
    "        cat3=torch.cat((x1,x2,x3,x4),1)\n",
    "        k=F.relu(self.conv5(cat3))\n",
    "        output=k*x-k+self.b\n",
    "        return F.relu(output)\n",
    "class FinalCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FinalCNN, self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv3=nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv4=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv5=nn.Conv2d(in_channels=64,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv6=nn.Conv2d(in_channels=32,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv7=nn.Conv2d(in_channels=16,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1=self.relu(self.conv1(x))\n",
    "        x2=self.relu(self.conv2(x1))\n",
    "        x3=self.relu(self.conv3(x2))\n",
    "        x4=self.relu(self.conv4(x3))\n",
    "        x5=self.relu(self.conv5(x4))+x3\n",
    "        x6=self.relu(self.conv6(x5))+x1\n",
    "        x7=self.relu(self.conv7(x6)) \n",
    "        return self.relu(x7*x-x7+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b204b",
   "metadata": {
    "papermill": {
     "duration": 0.005918,
     "end_time": "2024-08-04T13:37:39.692361",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.686443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb40054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:37:39.706278Z",
     "iopub.status.busy": "2024-08-04T13:37:39.705855Z",
     "iopub.status.idle": "2024-08-04T13:37:39.802893Z",
     "shell.execute_reply": "2024-08-04T13:37:39.801657Z"
    },
    "papermill": {
     "duration": 0.107468,
     "end_time": "2024-08-04T13:37:39.805931",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.698463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Successfully..\n"
     ]
    }
   ],
   "source": [
    "dwt_dehazenet_rgb=nn.DataParallel(DWT_DehazingNet())\n",
    "dehazenet_rgb=nn.DataParallel(DehazingNet())\n",
    "final_cnn=nn.DataParallel(FinalCNN())\n",
    "\n",
    "\n",
    "dwt_dehazenet_rgb.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/dehazing-rgb-dwt-2l.pth',map_location=torch.device('cpu')))\n",
    "dehazenet_rgb.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/dehazenet-rgb-2l.pth',map_location=torch.device('cpu')))\n",
    "final_cnn.load_state_dict(torch.load(r'/kaggle/input/dehazing-models-ct5129/end_cnn.pth',map_location=torch.device('cpu')))\n",
    "print('Loaded Successfully..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce9aed1",
   "metadata": {
    "papermill": {
     "duration": 0.006056,
     "end_time": "2024-08-04T13:37:39.818431",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.812375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utility Functions for the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a559263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:37:39.833033Z",
     "iopub.status.busy": "2024-08-04T13:37:39.832622Z",
     "iopub.status.idle": "2024-08-04T13:37:39.855152Z",
     "shell.execute_reply": "2024-08-04T13:37:39.854052Z"
    },
    "papermill": {
     "duration": 0.03302,
     "end_time": "2024-08-04T13:37:39.857675",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.824655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_image_rgb(img_tensor,file_path):\n",
    "    if img_tensor.shape[0]!=3:\n",
    "        raise ValueError(\"Input tensor must have 3 channels only...\")\n",
    "    rgb_array=tensor_denormalize_rgb(img_tensor).permute(1,2,0).cpu().detach().numpy()\n",
    "    rgb_image=Image.fromarray((np.clip(rgb_array,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    rgb_image.save(file_path)\n",
    "\n",
    "def tensor_denormalize_rgb(out_tensor,mean=[0.4556,0.3837,0.3642],std=[0.2689,0.2691,0.2828]):\n",
    "    if len(out_tensor.shape)==3:\n",
    "        out_tensor=out_tensor.unsqueeze(0)\n",
    "    mean=torch.tensor(mean).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "    std=torch.tensor(std).unsqueeze(0).unsqueeze(2).unsqueeze(3)    \n",
    "    denorm_tensor=(out_tensor*std)+mean\n",
    "    return denorm_tensor.squeeze(0)\n",
    "def unsharp_mask(image,kernel_size=(5,5),sigma=0.4,amount=1.0,threshold=1):\n",
    "    blurred=cv2.GaussianBlur(image,kernel_size,sigma)\n",
    "    sharpened=float(amount+1)*image-float(amount)*blurred\n",
    "    sharpened=np.maximum(sharpened,np.zeros(sharpened.shape))\n",
    "    sharpened=np.minimum(sharpened,255*np.ones(sharpened.shape))\n",
    "    sharpened=sharpened.round().astype(np.uint8)\n",
    "    if threshold>0:\n",
    "        low_contrast_mask=np.absolute(image-blurred)<threshold\n",
    "        np.copyto(sharpened,image,where=low_contrast_mask)\n",
    "    return sharpened\n",
    "\n",
    "def clahe(image):\n",
    "    clahe=cv2.createCLAHE(clipLimit=1,tileGridSize=(2,2))\n",
    "    lab=cv2.cvtColor(image,cv2.COLOR_BGR2LAB)\n",
    "    lab[:,:,0]=clahe.apply(lab[:,:,0])\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def enhance_image(image_path):\n",
    "    image=cv2.imread(image_path)\n",
    "    img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_sharpened=unsharp_mask(img)\n",
    "    image_clahe=clahe(image_sharpened)\n",
    "    image_tensor=tt.ToTensor()(image_clahe)   \n",
    "    return image_tensor\n",
    "def alpha_blending(image1, image2, alpha):\n",
    "    blended = cv2.addWeighted(image1, alpha, image2, 1 - alpha, 4)\n",
    "    return blended\n",
    "def image_addition(coeff,img_path1,img_path2):\n",
    "    img1=cv2.cvtColor(cv2.imread(img_path1),cv2.COLOR_BGR2RGB)\n",
    "    img2=cv2.cvtColor(cv2.imread(img_path2),cv2.COLOR_BGR2RGB)\n",
    "    img_f=tt.ToTensor()(alpha_blending(img1,img2,coeff))    \n",
    "    return img_f\n",
    "def save_image_final(img_tensor,file_path):\n",
    "    if img_tensor.shape[0]!=3:\n",
    "        raise ValueError(\"Input tensor must have 3 channels only...\")\n",
    "    rgb_array=img_tensor.permute(1,2,0).cpu().detach().numpy()\n",
    "    rgb_image=Image.fromarray((np.clip(rgb_array,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    rgb_image.save(file_path)\n",
    "def enhance_image_merged(image_path):\n",
    "    image=cv2.imread(image_path)\n",
    "    img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img2=cv2.fastNlMeansDenoisingColored(img,None,10,10,3,21)\n",
    "    image_sharpened=unsharp_mask(img2)\n",
    "    image_clahe=clahe(image_sharpened)\n",
    "    image_tensor=tt.ToTensor()(image_clahe)   \n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0f5aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:37:39.872109Z",
     "iopub.status.busy": "2024-08-04T13:37:39.871701Z",
     "iopub.status.idle": "2024-08-04T13:37:39.893536Z",
     "shell.execute_reply": "2024-08-04T13:37:39.892199Z"
    },
    "papermill": {
     "duration": 0.03221,
     "end_time": "2024-08-04T13:37:39.896223",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.864013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "psnr_fn,ssim_fn=PeakSignalNoiseRatio(),StructuralSimilarityIndexMeasure()\n",
    "def metrics_calculator(out_path,clear_path):\n",
    "    out_tensor=tt.ToTensor()(cv2.imread(out_path))\n",
    "    clear_tensor=tt.ToTensor()(cv2.resize(cv2.imread(clear_path),(256,256)))\n",
    "    return psnr_fn(out_tensor,clear_tensor),ssim_fn(out_tensor.unsqueeze(0),clear_tensor.unsqueeze(0))\n",
    "def averager(li):\n",
    "    return sum(li)/len(li)\n",
    "\n",
    "def model1_image_pass(img_tensor,folder_name,i,clear_path):\n",
    "    model_output1=dwt_dehazenet_rgb(img_tensor.unsqueeze(0)).cpu()\n",
    "    save_image_rgb(model_output1.squeeze(),f'/kaggle/working/{folder_name}/output_image_{i}_1.png')\n",
    "    psnr1_1,ssim1_1=metrics_calculator(f'/kaggle/working/{folder_name}/output_image_{i}_1.png',clear_path)\n",
    "    proc_out_tensor1=enhance_image(f'/kaggle/working/{folder_name}/output_image_{i}_1.png')\n",
    "    proc_arr1=proc_out_tensor1.permute(1,2,0).cpu().detach().numpy()\n",
    "    proc_image1=Image.fromarray((np.clip(proc_arr1,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    proc_image1.save(f'/kaggle/working/{folder_name}/processed_image_{i}_1.png')\n",
    "    psnr2_1,ssim_2_1=metrics_calculator(f'/kaggle/working/{folder_name}/processed_image_{i}_1.png',clear_path)\n",
    "    return proc_arr1,psnr1_1,ssim1_1,psnr2_1,ssim_2_1\n",
    "    \n",
    "    \n",
    "def model2_image_pass(img_tensor,folder_name,i,clear_path):\n",
    "    model_output2=dehazenet_rgb(img_tensor.unsqueeze(0)).cpu()\n",
    "    save_image_rgb(model_output2.squeeze(),f'/kaggle/working/{folder_name}/output_image_{i}_2.png')\n",
    "    psnr1_2,ssim1_2=metrics_calculator(f'/kaggle/working/{folder_name}/output_image_{i}_2.png',clear_path)\n",
    "    proc_out_tensor2=enhance_image(f'/kaggle/working/{folder_name}/output_image_{i}_2.png')\n",
    "    proc_arr2=proc_out_tensor2.permute(1,2,0).cpu().detach().numpy()\n",
    "    proc_image2=Image.fromarray((np.clip(proc_arr2,0,1)*255).astype(np.uint8),mode='RGB')\n",
    "    proc_image2.save(f'/kaggle/working/{folder_name}/processed_image_{i}_2.png')\n",
    "    psnr2_2,ssim_2_2=metrics_calculator(f'/kaggle/working/{folder_name}/processed_image_{i}_2.png',clear_path)\n",
    "    psnr,ssim=metrics_calculator(f'/kaggle/working/{folder_name}/processed_image_{i}_2.png',clear_path)\n",
    "    return proc_arr2,psnr1_2,ssim1_2,psnr2_2,ssim_2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c1cb9",
   "metadata": {
    "papermill": {
     "duration": 0.005954,
     "end_time": "2024-08-04T13:37:39.908367",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.902413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Complete Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a109ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:37:39.923420Z",
     "iopub.status.busy": "2024-08-04T13:37:39.922444Z",
     "iopub.status.idle": "2024-08-04T13:37:48.664218Z",
     "shell.execute_reply": "2024-08-04T13:37:48.662753Z"
    },
    "papermill": {
     "duration": 8.752295,
     "end_time": "2024-08-04T13:37:48.667098",
     "exception": false,
     "start_time": "2024-08-04T13:37:39.914803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def model_run_parallel(haze_img_path,clear_img_path,coeff=0.6,i=0,folder_name='Inference_Results'):\n",
    "    os.makedirs(f'/kaggle/working/{folder_name}',exist_ok=True)\n",
    "    dwt_dehazenet_rgb.eval()\n",
    "    dwt_dehazenet_rgb.eval()\n",
    "    start_time=time.time()\n",
    "    haze_img_tensor=cv2.cvtColor(cv2.imread(haze_img_path),cv2.COLOR_BGR2RGB)\n",
    "    haze_img_tensor=Image.open(haze_img_path)\n",
    "    img_tensor=input_transforms_rgb(haze_img_tensor).cpu()\n",
    "    out1,out2=Parallel(n_jobs=2)(delayed(func)(img_tensor,folder_name,i,clear_img_path) for func in [model1_image_pass,model2_image_pass])\n",
    "    #out1,out2=model1_image_pass(img_tensor,folder_name,i,clear_img_path),model2_image_pass(img_tensor,folder_name,i,clear_img_path)\n",
    "    psnr_mod11,ssim_mod11,psnr_mod21,ssim_mod21,psnr_mod12,ssim_mod12,psnr_mod22,ssim_mod22=out1[1],out1[2],out1[3],out1[4],out2[1],out2[2],out2[3],out2[4]\n",
    "    blended_image=image_addition(coeff,f'/kaggle/working/{folder_name}/processed_image_{i}_2.png',\n",
    "                                 f'/kaggle/working/{folder_name}/processed_image_{i}_1.png')\n",
    "    save_image_final(blended_image.squeeze(),f'/kaggle/working/{folder_name}/merged_image_{i}.png')\n",
    "    psnr_blend,ssim_blend=metrics_calculator(f'/kaggle/working/{folder_name}/merged_image_{i}.png',clear_img_path)\n",
    "    \n",
    "    proc_blended=enhance_image_merged(f'/kaggle/working/{folder_name}/merged_image_{i}.png')\n",
    "    save_image_final(blended_image.squeeze(),f'/kaggle/working/{folder_name}/proc_merged_image_{i}.png')\n",
    "    psnr_procblend,ssim_procblend=metrics_calculator(f'/kaggle/working/{folder_name}/proc_merged_image_{i}.png',clear_img_path)\n",
    "    \n",
    "    cnn_processed=final_cnn(tt.ToTensor()(cv2.cvtColor(cv2.imread(f'/kaggle/working/{folder_name}/proc_merged_image_{i}.png'),\n",
    "                                                       cv2.COLOR_BGR2RGB)))\n",
    "    save_image_final(cnn_processed.squeeze(),f'/kaggle/working/{folder_name}/cnn_processed_image_{i}.png')\n",
    "    psnr_cnn,ssim_cnn=metrics_calculator(f'/kaggle/working/{folder_name}/cnn_processed_image_{i}.png',clear_img_path)\n",
    "    return psnr_mod11,ssim_mod11,psnr_mod21,ssim_mod21,psnr_mod12,ssim_mod12,psnr_mod22,ssim_mod22,psnr_blend,ssim_blend,psnr_procblend,ssim_procblend,psnr_cnn,ssim_cnn\n",
    "\n",
    "out=model_run_parallel('/kaggle/input/dehazing-dataset-thesis/BeDDE/BeDDE/beijing/fog/beijing_1.png','/kaggle/input/dehazing-dataset-thesis/BeDDE/BeDDE/beijing/gt/beijing_clear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd4b1b",
   "metadata": {
    "papermill": {
     "duration": 0.006262,
     "end_time": "2024-08-04T13:37:48.680497",
     "exception": false,
     "start_time": "2024-08-04T13:37:48.674235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preparing BeDDE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e959fc11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:37:48.695114Z",
     "iopub.status.busy": "2024-08-04T13:37:48.694656Z",
     "iopub.status.idle": "2024-08-04T13:37:49.072602Z",
     "shell.execute_reply": "2024-08-04T13:37:49.071318Z"
    },
    "papermill": {
     "duration": 0.388701,
     "end_time": "2024-08-04T13:37:49.075472",
     "exception": false,
     "start_time": "2024-08-04T13:37:48.686771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GT</th>\n",
       "      <th>Hazy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/dehazing-dataset-thesis/BeDDE/Be...</td>\n",
       "      <td>/kaggle/input/dehazing-dataset-thesis/BeDDE/Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/dehazing-dataset-thesis/BeDDE/Be...</td>\n",
       "      <td>/kaggle/input/dehazing-dataset-thesis/BeDDE/Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/dehazing-dataset-thesis/BeDDE/Be...</td>\n",
       "      <td>/kaggle/input/dehazing-dataset-thesis/BeDDE/Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/dehazing-dataset-thesis/BeDDE/Be...</td>\n",
       "      <td>/kaggle/input/dehazing-dataset-thesis/BeDDE/Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/dehazing-dataset-thesis/BeDDE/Be...</td>\n",
       "      <td>/kaggle/input/dehazing-dataset-thesis/BeDDE/Be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  GT  \\\n",
       "0  /kaggle/input/dehazing-dataset-thesis/BeDDE/Be...   \n",
       "1  /kaggle/input/dehazing-dataset-thesis/BeDDE/Be...   \n",
       "2  /kaggle/input/dehazing-dataset-thesis/BeDDE/Be...   \n",
       "3  /kaggle/input/dehazing-dataset-thesis/BeDDE/Be...   \n",
       "4  /kaggle/input/dehazing-dataset-thesis/BeDDE/Be...   \n",
       "\n",
       "                                                Hazy  \n",
       "0  /kaggle/input/dehazing-dataset-thesis/BeDDE/Be...  \n",
       "1  /kaggle/input/dehazing-dataset-thesis/BeDDE/Be...  \n",
       "2  /kaggle/input/dehazing-dataset-thesis/BeDDE/Be...  \n",
       "3  /kaggle/input/dehazing-dataset-thesis/BeDDE/Be...  \n",
       "4  /kaggle/input/dehazing-dataset-thesis/BeDDE/Be...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazy_images=list(glob('/kaggle/input/dehazing-dataset-thesis/BeDDE/BeDDE/*/fog/*'))\n",
    "hazy_images.sort()\n",
    "clear_images=list(map(lambda x:x.split('_')[0]+'_clear.png',hazy_images))\n",
    "clear_images=list(map(lambda x:'/'.join(x.split('/')[:7])+'/gt/'+x.split('/')[-1],clear_images))\n",
    "clear_images.sort()\n",
    "tabular_data_bedde=pd.DataFrame({'GT':clear_images,\n",
    "                                     'Hazy':hazy_images})\n",
    "tabular_data_bedde.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7befae",
   "metadata": {
    "papermill": {
     "duration": 0.006925,
     "end_time": "2024-08-04T13:37:49.089269",
     "exception": false,
     "start_time": "2024-08-04T13:37:49.082344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Procuring Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c872538",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T13:37:49.104082Z",
     "iopub.status.busy": "2024-08-04T13:37:49.103703Z",
     "iopub.status.idle": "2024-08-04T13:42:51.653330Z",
     "shell.execute_reply": "2024-08-04T13:42:51.652037Z"
    },
    "papermill": {
     "duration": 302.565961,
     "end_time": "2024-08-04T13:42:51.661860",
     "exception": false,
     "start_time": "2024-08-04T13:37:49.095899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed....\n",
      "Average workflow processing time: 1.4544073583988042\n",
      "Average metrics at DWT Dehazernet: 13.424005508422852 and 0.6109381318092346\n",
      "Average metrics at Postprocessing DWT Dehazernet: 13.504715919494629 and 0.6310338377952576\n",
      "Average metrics at Dehazernet: 13.484880447387695 and 0.6228863000869751\n",
      "Average metrics at Postprocessing Dehazernet: 13.704506874084473 and 0.649851381778717\n",
      "Average metrics at Blending: 13.835047721862793 and 0.6510834097862244\n",
      "Average metrics at Postprocessing Blending: 13.835047721862793 and 0.6510834097862244\n",
      "Average metrics at Enhancer CNN: 9.76932144165039 and 0.5548993945121765\n"
     ]
    }
   ],
   "source": [
    "time_data,psnr_mod11,ssim_mod11,psnr_mod21,ssim_mod21,psnr_mod12,ssim_mod12,psnr_mod22,ssim_mod22,psnr_blend,ssim_blend,psnr_procblend,ssim_procblend,psnr_cnn,ssim_cnn=[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "for idx,(clear,hazy) in enumerate(tabular_data_bedde.values):\n",
    "    coeff=0.9\n",
    "    start_time=time.time()\n",
    "    outputs=model_run_parallel(hazy,clear,coeff,idx)\n",
    "    time_data.append(time.time()-start_time)\n",
    "    psnr_mod11.append(outputs[0])\n",
    "    ssim_mod11.append(outputs[1])\n",
    "    psnr_mod21.append(outputs[2])\n",
    "    ssim_mod21.append(outputs[3])\n",
    "    psnr_mod12.append(outputs[4])\n",
    "    ssim_mod12.append(outputs[5])\n",
    "    psnr_mod22.append(outputs[6])\n",
    "    ssim_mod22.append(outputs[7])\n",
    "    psnr_blend.append(outputs[8])\n",
    "    ssim_blend.append(outputs[9])\n",
    "    psnr_procblend.append(outputs[10])\n",
    "    ssim_procblend.append(outputs[11])\n",
    "    psnr_cnn.append(outputs[12])\n",
    "    ssim_cnn.append(outputs[13])\n",
    "\n",
    "print('Processed....')\n",
    "print(f\"Average workflow processing time: {averager(time_data)}\")\n",
    "print(f\"Average metrics at DWT Dehazernet: {averager(psnr_mod11)} and {averager(ssim_mod11)}\")\n",
    "print(f\"Average metrics at Postprocessing DWT Dehazernet: {averager(psnr_mod21)} and {averager(ssim_mod21)}\")\n",
    "print(f\"Average metrics at Dehazernet: {averager(psnr_mod12)} and {averager(ssim_mod12)}\")\n",
    "print(f\"Average metrics at Postprocessing Dehazernet: {averager(psnr_mod22)} and {averager(ssim_mod22)}\")\n",
    "print(f\"Average metrics at Blending: {averager(psnr_blend)} and {averager(ssim_blend)}\")\n",
    "print(f\"Average metrics at Postprocessing Blending: {averager(psnr_procblend)} and {averager(ssim_procblend)}\")\n",
    "print(f\"Average metrics at Enhancer CNN: {averager(psnr_cnn)} and {averager(ssim_cnn)}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5101434,
     "sourceId": 8729607,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5239863,
     "sourceId": 8857586,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 348.325898,
   "end_time": "2024-08-04T13:42:54.290418",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-04T13:37:05.964520",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
